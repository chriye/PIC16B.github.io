---
layout: post
title: Blog Post 6
---

Yeah, welcome back! Before we talk about the topic in this blog post. Let's start with fake news. What's fake news? It's false information that presented as news, which may mislead people and cause damaging. So having a model. that can identify fake news is needed!! And our task is create a model that can classify fake new using tensorflow.

During this blog post, we recommand using google colab.

## Set up

These're all the packges that we'll need later.


```python
# read and convert data to dataframe
import pandas as pd

# stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

# tensorflow
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# plot
from matplotlib import pyplot as plt
import plotly.express as px 
import numpy as np
from plotly.io import write_html

# model
import re
import string

# pca
from sklearn.decomposition import PCA



```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!


## 1. Acquire Training Data

The following code block is accessed from Kaggle. And the data has been clean up.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
```

Taking a look at the data, each row is a new information, which includes its title, text content, and label of if it's a fake new. In 'fake' column, **0** indicates the new is true and **1** means it's fake.


```python
df
```



## 2. Make a Dataset

We'll write a function call make_dataset. The function replaces stopwords in text and tile with " ". And it also returns a tf.data.Dataset with two inputs("title", and "text") and one output("fake").

Before we deep jump into more details on code, let's introduce what are stopwords. Stopwords are "and", "the", "but" etc.

Python provides a packages that contains all the stopwords. You'll need to run the next 4 commands. If you ran these commands already, you may skip running these.

```python
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
```

At the end of function, I recommand you to do `data.batch(100)` before returning the data.


```python
def make_dataset(dataframe):
  """
  input  : dataframe
  output : a tf.data.Dataset with two inputs(title and text columns, which has removed stopwords)
           and a output which is fake column.
  """
  # replace the stopwords in text
  dataframe['text'] = dataframe['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # replace the stopwords in title
  dataframe['title'] = dataframe['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # create a tf.data.Dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        # input
        {
            "title": dataframe[["title"]],
            "text": dataframe[["text"]]
        },
        # output
        {
            "fake": dataframe[["fake"]]
        }
    )
  )
  data = data.batch(100)
  return data
```

Call the make_dataset with df input and save it into df.


```python
df = make_dataset(df)
```

Before we train and split, we'll shuffle the dataframe. And then, we'll split the df data in to train and test, which trainning data is 80% and testing data is 20% of df.


```python
# shuffle
df = df.shuffle(buffer_size = len(df))
# sizes for traning and testing
train_size = int(0.8*len(df))
val_size = int(0.2*len(df))

# traning data and testing data
train = df.take(train_size)
val = df.skip(train_size).take(val_size)
```

#### *Base rate*
By looking at the labels on the training set, a model has 50% of properties say that is fake news!

## 2. Creating Models

In this part, we'll create three models.

- create a model that has only title column as a input

- create a model that has only text column as a input

- create a model that has title and text columns as inputs


The purpose of creating three models is this may tell which model will perform better. 

Before we start it, few tips are given:

- Use Functional API instead of Sequential API. 

  *View*:
  - *Sequential API is one input to one output.*
  - *Functional API is multiple inputs to one output.*

- In order to reduce the overfitting, you may want to add *Dropout* layer. 

<br>

We first start with writing a function that standadize the data, which is called standardization(). What it does is convert all characters into lower case and replace the punctuations with " " in strings.

Then, we write a model TextVectorization that counts frequency of words and then replace words by their freqency's ranking. 


```python
# maximum for frequency's ranking
size_vocabulary = 2000

# convert characters into lower case and replace punctuation in strings
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

# counts frequency for each words and replace words into their frequency ranking
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

# apply the title column to model
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

Declare each input before we use it. For inputs, since each new has a title and a text, the **shape** will be (1,). The reason why written in (1,) instead of 1 is shapes should be written in a tuple form. And **name** will be their corresponding columns in the df. **dtype** will be their type.


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

Since we'll use **Embedding** for title and text, we may write one line and share it. 

Notes:

- `layer.Embedding(input dim, output dim, *args)`



```python
shared_embedding = layers.Embedding(size_vocabulary, 2, name = "embedding")
```

Review, in the last blog post we talk about model is all different layers stack on top of each other. And the following next two blocks are parts of models for the three models. Later, we'll modify them later for each model.


```python
title_features = vectorize_layer(title_input)
title_features = shared_embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```


```python
text_features = vectorize_layer(text_input)
text_features = shared_embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

## Model 1: title as input only

Since the output of fake will be either 1 or 0, we add one more Dense layer on title_features.


```python
output1 = layers.Dense(2, name = "fake")(title_features)
```

Create the model1 that has title_input for inputs, and output1 for outputs.


```python
model1 = keras.Model(
    inputs = title_input,
    outputs = output1
)
```


```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 2)            4000      
                                                                     
     dropout (Dropout)           (None, 500, 2)            0         
                                                                     
     global_average_pooling1d (G  (None, 2)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 2)                 0         
                                                                     
     dense (Dense)               (None, 32)                96        
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 4,162
    Trainable params: 4,162
    Non-trainable params: 0
    _________________________________________________________________



```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train model 1, and review its validation accuracy.


```python
history1 = model1.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 10ms/step - loss: 0.6922 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5153
    Epoch 2/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.6899 - accuracy: 0.5257 - val_loss: 0.6850 - val_accuracy: 0.5196
    Epoch 3/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.6574 - accuracy: 0.6913 - val_loss: 0.6080 - val_accuracy: 0.8333
    Epoch 4/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.5359 - accuracy: 0.8714 - val_loss: 0.4455 - val_accuracy: 0.9480
    Epoch 5/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.3928 - accuracy: 0.9096 - val_loss: 0.3121 - val_accuracy: 0.9438
    Epoch 6/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.3003 - accuracy: 0.9213 - val_loss: 0.2307 - val_accuracy: 0.9483
    Epoch 7/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.2439 - accuracy: 0.9262 - val_loss: 0.1802 - val_accuracy: 0.9598
    Epoch 8/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.2070 - accuracy: 0.9347 - val_loss: 0.1466 - val_accuracy: 0.9640
    Epoch 9/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1772 - accuracy: 0.9416 - val_loss: 0.1284 - val_accuracy: 0.9716
    Epoch 10/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1618 - accuracy: 0.9446 - val_loss: 0.1125 - val_accuracy: 0.9693
    Epoch 11/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1503 - accuracy: 0.9452 - val_loss: 0.0943 - val_accuracy: 0.9782
    Epoch 12/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1383 - accuracy: 0.9473 - val_loss: 0.0937 - val_accuracy: 0.9751
    Epoch 13/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1304 - accuracy: 0.9518 - val_loss: 0.0762 - val_accuracy: 0.9786
    Epoch 14/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1218 - accuracy: 0.9509 - val_loss: 0.0831 - val_accuracy: 0.9722
    Epoch 15/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1155 - accuracy: 0.9555 - val_loss: 0.0658 - val_accuracy: 0.9809
    Epoch 16/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1128 - accuracy: 0.9538 - val_loss: 0.0689 - val_accuracy: 0.9756
    Epoch 17/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1089 - accuracy: 0.9553 - val_loss: 0.0629 - val_accuracy: 0.9809
    Epoch 18/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1074 - accuracy: 0.9551 - val_loss: 0.0596 - val_accuracy: 0.9811
    Epoch 19/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.1035 - accuracy: 0.9583 - val_loss: 0.0608 - val_accuracy: 0.9784
    Epoch 20/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1012 - accuracy: 0.9575 - val_loss: 0.0631 - val_accuracy: 0.9793
    Epoch 21/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0984 - accuracy: 0.9570 - val_loss: 0.0515 - val_accuracy: 0.9829
    Epoch 22/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0944 - accuracy: 0.9582 - val_loss: 0.0565 - val_accuracy: 0.9800
    Epoch 23/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0933 - accuracy: 0.9588 - val_loss: 0.0501 - val_accuracy: 0.9838
    Epoch 24/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.0928 - accuracy: 0.9586 - val_loss: 0.0491 - val_accuracy: 0.9840
    Epoch 25/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0909 - accuracy: 0.9602 - val_loss: 0.0407 - val_accuracy: 0.9869
    Epoch 26/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.0878 - accuracy: 0.9596 - val_loss: 0.0397 - val_accuracy: 0.9871
    Epoch 27/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0858 - accuracy: 0.9613 - val_loss: 0.0445 - val_accuracy: 0.9849
    Epoch 28/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.0875 - accuracy: 0.9592 - val_loss: 0.0408 - val_accuracy: 0.9867
    Epoch 29/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.0876 - accuracy: 0.9603 - val_loss: 0.0361 - val_accuracy: 0.9884
    Epoch 30/30
    180/180 [==============================] - 2s 9ms/step - loss: 0.0864 - accuracy: 0.9619 - val_loss: 0.0384 - val_accuracy: 0.9867


To easier view the accuracy at each epoch, we may made a plot on it.


```python
plt.plot(history1.history["accuracy"], label = "training")
plt.plot(history1.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f17733e7c10>


  
![output_31_1.png]({{site.baseurl}}/images/output_31_1.png) 
    


From the plot, we may conclude that the model1 is not overfitting. Based on history1, the validating accuracy is about 98%.

## Model 2: text as input only

Also add one more Dense layer on text_features.


```python
output2 = layers.Dense(2, name = "fake")(text_features)
```

Create a model2 that has text_input for inputs, and output2 for outputs.


```python
model2 = keras.Model(
    inputs = text_input,
    outputs = output2
)
```


```python
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 2)            4000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 2)            0         
                                                                     
     global_average_pooling1d_1   (None, 2)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 2)                 0         
                                                                     
     dense_1 (Dense)             (None, 32)                96        
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 4,162
    Trainable params: 4,162
    Non-trainable params: 0
    _________________________________________________________________



```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train the data and review the history2.


```python
history2 = model2.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 4s 16ms/step - loss: 0.6234 - accuracy: 0.6878 - val_loss: 0.5333 - val_accuracy: 0.8101
    Epoch 2/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.4643 - accuracy: 0.8208 - val_loss: 0.3511 - val_accuracy: 0.8980
    Epoch 3/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.3375 - accuracy: 0.8794 - val_loss: 0.2712 - val_accuracy: 0.9051
    Epoch 4/30
    180/180 [==============================] - 5s 28ms/step - loss: 0.2800 - accuracy: 0.9025 - val_loss: 0.2294 - val_accuracy: 0.9373
    Epoch 5/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.2525 - accuracy: 0.9117 - val_loss: 0.2155 - val_accuracy: 0.9316
    Epoch 6/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.2362 - accuracy: 0.9213 - val_loss: 0.2036 - val_accuracy: 0.9400
    Epoch 7/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.2208 - accuracy: 0.9262 - val_loss: 0.1852 - val_accuracy: 0.9320
    Epoch 8/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.2065 - accuracy: 0.9284 - val_loss: 0.1720 - val_accuracy: 0.9476
    Epoch 9/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.2052 - accuracy: 0.9311 - val_loss: 0.1585 - val_accuracy: 0.9571
    Epoch 10/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1912 - accuracy: 0.9345 - val_loss: 0.1437 - val_accuracy: 0.9627
    Epoch 11/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1853 - accuracy: 0.9350 - val_loss: 0.1599 - val_accuracy: 0.9576
    Epoch 12/30
    180/180 [==============================] - 5s 28ms/step - loss: 0.1798 - accuracy: 0.9383 - val_loss: 0.1501 - val_accuracy: 0.9607
    Epoch 13/30
    180/180 [==============================] - 6s 31ms/step - loss: 0.1775 - accuracy: 0.9401 - val_loss: 0.1523 - val_accuracy: 0.9593
    Epoch 14/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1720 - accuracy: 0.9420 - val_loss: 0.1417 - val_accuracy: 0.9651
    Epoch 15/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1639 - accuracy: 0.9416 - val_loss: 0.1429 - val_accuracy: 0.9569
    Epoch 16/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1568 - accuracy: 0.9449 - val_loss: 0.1302 - val_accuracy: 0.9667
    Epoch 17/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1617 - accuracy: 0.9438 - val_loss: 0.1312 - val_accuracy: 0.9658
    Epoch 18/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1508 - accuracy: 0.9474 - val_loss: 0.1173 - val_accuracy: 0.9713
    Epoch 19/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1479 - accuracy: 0.9475 - val_loss: 0.1131 - val_accuracy: 0.9702
    Epoch 20/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1450 - accuracy: 0.9468 - val_loss: 0.1066 - val_accuracy: 0.9724
    Epoch 21/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1444 - accuracy: 0.9500 - val_loss: 0.1101 - val_accuracy: 0.9691
    Epoch 22/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1352 - accuracy: 0.9516 - val_loss: 0.1035 - val_accuracy: 0.9759
    Epoch 23/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1364 - accuracy: 0.9512 - val_loss: 0.1099 - val_accuracy: 0.9713
    Epoch 24/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1364 - accuracy: 0.9511 - val_loss: 0.0986 - val_accuracy: 0.9744
    Epoch 25/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1341 - accuracy: 0.9539 - val_loss: 0.1064 - val_accuracy: 0.9716
    Epoch 26/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1287 - accuracy: 0.9541 - val_loss: 0.0924 - val_accuracy: 0.9751
    Epoch 27/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1287 - accuracy: 0.9528 - val_loss: 0.0833 - val_accuracy: 0.9744
    Epoch 28/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1283 - accuracy: 0.9539 - val_loss: 0.0915 - val_accuracy: 0.9771
    Epoch 29/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1211 - accuracy: 0.9556 - val_loss: 0.0906 - val_accuracy: 0.9751
    Epoch 30/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1256 - accuracy: 0.9532 - val_loss: 0.0789 - val_accuracy: 0.9807


To easily view the accuracy at each epoch, we may made a plot on it.


```python
plt.plot(history2.history["accuracy"], label = "training")
plt.plot(history2.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1776ac7390>




    
![output_42_1.png]({{site.baseurl}}/images/output_42_1.png) 



From the plot, model2 is not overfitting as well. And base on the history2, the vilidating accuracy is about 97%.

## Model 3: title and text as input

Since we have two distinct inputs, we need to concatenate them and them apply two more Dense layer on them.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```

Create a model3 that has title_input, and text_input for inputs and output for outputs.


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['title[0][0]',                  
     ization)                                                         'text[0][0]']                   
                                                                                                      
     embedding (Embedding)          (None, 500, 2)       4000        ['text_vectorization[0][0]',     
                                                                      'text_vectorization[1][0]']     
                                                                                                      
     dropout (Dropout)              (None, 500, 2)       0           ['embedding[0][0]']              
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 2)       0           ['embedding[1][0]']              
                                                                                                      
     global_average_pooling1d (Glob  (None, 2)           0           ['dropout[0][0]']                
     alAveragePooling1D)                                                                              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 2)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_1 (Dropout)            (None, 2)            0           ['global_average_pooling1d[0][0]'
                                                                     ]                                
                                                                                                      
     dropout_3 (Dropout)            (None, 2)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dense (Dense)                  (None, 32)           96          ['dropout_1[0][0]']              
                                                                                                      
     dense_1 (Dense)                (None, 32)           96          ['dropout_3[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  
                                                                      'dense_1[0][0]']                
                                                                                                      
     dense_2 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_2[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 6,338
    Trainable params: 6,338
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train the model3 and review history3.


```python
history3 = model3.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30
    180/180 [==============================] - 5s 20ms/step - loss: 0.2325 - accuracy: 0.9563 - val_loss: 0.0515 - val_accuracy: 0.9889
    Epoch 2/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0540 - accuracy: 0.9854 - val_loss: 0.0307 - val_accuracy: 0.9911
    Epoch 3/30
    180/180 [==============================] - 4s 24ms/step - loss: 0.0433 - accuracy: 0.9868 - val_loss: 0.0226 - val_accuracy: 0.9933
    Epoch 4/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.0211 - val_accuracy: 0.9938
    Epoch 5/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.0258 - val_accuracy: 0.9918
    Epoch 6/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0350 - accuracy: 0.9880 - val_loss: 0.0184 - val_accuracy: 0.9931
    Epoch 7/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.0198 - val_accuracy: 0.9920
    Epoch 8/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0306 - accuracy: 0.9897 - val_loss: 0.0210 - val_accuracy: 0.9929
    Epoch 9/30
    180/180 [==============================] - 4s 19ms/step - loss: 0.0323 - accuracy: 0.9887 - val_loss: 0.0116 - val_accuracy: 0.9967
    Epoch 10/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0325 - accuracy: 0.9886 - val_loss: 0.0201 - val_accuracy: 0.9929
    Epoch 11/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0315 - accuracy: 0.9896 - val_loss: 0.0146 - val_accuracy: 0.9944
    Epoch 12/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0305 - accuracy: 0.9891 - val_loss: 0.0133 - val_accuracy: 0.9960
    Epoch 13/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0278 - accuracy: 0.9899 - val_loss: 0.0158 - val_accuracy: 0.9944
    Epoch 14/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0294 - accuracy: 0.9895 - val_loss: 0.0130 - val_accuracy: 0.9962
    Epoch 15/30
    180/180 [==============================] - 4s 19ms/step - loss: 0.0276 - accuracy: 0.9903 - val_loss: 0.0121 - val_accuracy: 0.9951
    Epoch 16/30
    180/180 [==============================] - 4s 19ms/step - loss: 0.0301 - accuracy: 0.9889 - val_loss: 0.0133 - val_accuracy: 0.9969
    Epoch 17/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0297 - accuracy: 0.9894 - val_loss: 0.0139 - val_accuracy: 0.9964
    Epoch 18/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0270 - accuracy: 0.9900 - val_loss: 0.0080 - val_accuracy: 0.9971
    Epoch 19/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0283 - accuracy: 0.9899 - val_loss: 0.0140 - val_accuracy: 0.9962
    Epoch 20/30
    180/180 [==============================] - 4s 19ms/step - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.0139 - val_accuracy: 0.9949
    Epoch 21/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.0105 - val_accuracy: 0.9971
    Epoch 22/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0278 - accuracy: 0.9899 - val_loss: 0.0124 - val_accuracy: 0.9960
    Epoch 23/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0273 - accuracy: 0.9904 - val_loss: 0.0102 - val_accuracy: 0.9971
    Epoch 24/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.0131 - val_accuracy: 0.9949
    Epoch 25/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0268 - accuracy: 0.9897 - val_loss: 0.0140 - val_accuracy: 0.9962
    Epoch 26/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0260 - accuracy: 0.9902 - val_loss: 0.0085 - val_accuracy: 0.9982
    Epoch 27/30
    180/180 [==============================] - 4s 20ms/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.0089 - val_accuracy: 0.9971
    Epoch 28/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.0124 - val_accuracy: 0.9958
    Epoch 29/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0248 - accuracy: 0.9914 - val_loss: 0.0082 - val_accuracy: 0.9976
    Epoch 30/30
    180/180 [==============================] - 4s 21ms/step - loss: 0.0246 - accuracy: 0.9911 - val_loss: 0.0079 - val_accuracy: 0.9980


Made a plot to see how accuracy changes at each epoch.


```python
plt.plot(history3.history["accuracy"], label = "training")
plt.plot(history3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f177638e4d0>



![output_54_1.png]({{site.baseurl}}/images/output_54_1.png) 


From the plot, the traning line is below the validation line, which indicates the model3 is not overfitting. And the history3 shows that the accuracy is about 99.7%, which is barely close to 100%.

Comparing these three models, we can see that the model3 performs better, since it has accuracy close to 100%.

# 4. Model Evaluation

Download the following url for testing data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```


```python
test_df = pd.read_csv(test_url)
```

View on test_df.


```python
test_df
```



Call make_dataset on test_df to modify title and text columns.


```python
test_df = make_dataset(test_df)
```

Apply the best model3 on test_df.


```python
model3.evaluate(test_df)
```

    225/225 [==============================] - 2s 11ms/step - loss: 0.0295 - accuracy: 0.9908





    [0.029529914259910583, 0.9907791018486023]



Based on the model3 on testing data result, we have 99% of confidenct that we'll be right on predict fake news.

# 5. Embedding

Visualize on the embedding that our best model learned.


```python
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()   
```


```python
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

embedding_df
```

Use plot to visualize how words located.


```python
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))*2),
                 size_max = 4,
                 hover_name = "word")

# save the plot
write_html(fig, "embedding.html")
```
{% include embedding.html %}

On the right hand side of plot, we could see there're trump(8.75, 0.02), russias(5.21, 0.05), korea(3.77, 0.08), mayanmar(5.29, 0.1), chinas(4.07, 0.06). And these may be more related to fake news. The reason why this happens is that the topic related to political may easily mislead people.
