---
layout: post
title: Blog Post 6
---

Yeah, welcome back! Before we talk about the topic in this blog post. Let's start with fake news. What's fake news? It's false information that presented as news, which may mislead people and cause damaging. So having a model. that can identify fake news is needed!! And our task is create a model that can classify fake new using tensorflow.

During this blog post, we recommand using google colab.

## Set up

These're all the packges that we'll need later.


```python
# read and convert data to dataframe
import pandas as pd

# stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

# tensorflow
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# plot
from matplotlib import pyplot as plt

# model
import re
import string

```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.


## 1. Acquire Training Data

The following code block is accessed from Kaggle. And the data has been clean up.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
```

Taking a look at the data, each row is a new information, which includes its title, text content, and label of if it's a fake new. In 'fake' column, **0** indicates the new is true and **1** means it's fake.


```python
df
```





  <div id="df-a1b10302-6df7-4e09-9de9-6301fb708c88">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-a1b10302-6df7-4e09-9de9-6301fb708c88')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-a1b10302-6df7-4e09-9de9-6301fb708c88 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-a1b10302-6df7-4e09-9de9-6301fb708c88');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




## 2. Make a Dataset

We'll write a function call make_dataset. The function replaces stopwords in text and tile with " ". And it also returns a tf.data.Dataset with two inputs("title", and "text") and one output("fake").

Before we deep jump into more details on code, let's introduce what are stopwords. Stopwords are "and", "the", "but" etc.

Python provides a packages that contains all the stopwords. You'll need to run the next 4 commands. If you ran these commands already, you may skip running these.

```python
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
```

At the end of function, I recommand you to do `data.batch(100)` before returning the data.


```python
def make_dataset(dataframe):
  """
  input  : dataframe
  output : a tf.data.Dataset with two inputs(title and text columns, which has removed stopwords)
           and a output which is fake column.
  """
  # replace the stopwords in text
  dataframe['text'] = dataframe['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # replace the stopwords in title
  dataframe['title'] = dataframe['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # create a tf.data.Dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        # input
        {
            "title": dataframe[["title"]],
            "text": dataframe[["text"]]
        },
        # output
        {
            "fake": dataframe[["fake"]]
        }
    )
  )
  data = data.batch(100)
  return data
```

Call the make_dataset with df input and save it into df.


```python
df = make_dataset(df)
```

Before we train and split, we'll shuffle the dataframe. And then, we'll split the df data in to train and test, which trainning data is 80% and testing data is 20% of df.


```python
# shuffle
df = df.shuffle(buffer_size = len(df))
# sizes for traning and testing
train_size = int(0.8*len(df))
val_size = int(0.2*len(df))

# traning data and testing data
train = df.take(train_size)
val = df.skip(train_size).take(val_size)
```

#### *Base rate*
By looking at the labels on the training set, a model has 50% of properties say that is fake news!

## 2. Creating Models

In this part, we'll create three models.

- create a model that has only title column as a input

- create a model that has only text column as a input

- create a model that has title and text columns as inputs


The purpose of creating three models is this may tell which model will perform better. 

Before we start it, few tips are given:

- Use Functional API instead of Sequential API. 

  *View*:
  - *Sequential API is one input to one output.*
  - *Functional API is multiple inputs to one output.*

- In order to reduce the overfitting, you may want to add *Dropout* layer. 

<br>

We first start with writing a function that standadize the data, which is called standardization(). What it does is convert all characters into lower case and replace the punctuations with " " in strings.

Then, we write a model TextVectorization that counts frequency of words and then replace words by their freqency's ranking. 


```python
# maximum for frequency's ranking
size_vocabulary = 2000

# convert characters into lower case and replace punctuation in strings
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

# counts frequency for each words and replace words into their frequency ranking
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

# apply the title column to model
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

Declare each input before we use it. For inputs, since each new has a title and a text, the **shape** will be (1,). The reason why written in (1,) instead of 1 is shapes should be written in a tuple form. And **name** will be their corresponding columns in the df. **dtype** will be their type.


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

Since we'll use **Embedding** for title and text, we may write one line and share it. 

Notes:

- `layer.Embedding(input dim, output dim, *args)`



```python
shared_embedding = layers.Embedding(size_vocabulary, 2, name = "embedding")
```

Review, in the last blog post we talk about model is all different layers stack on top of each other. And the following next two blocks are parts of models for the three models. Later, we'll modify them later for each model.


```python
title_features = vectorize_layer(title_input)
title_features = shared_embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```


```python
text_features = vectorize_layer(text_input)
text_features = shared_embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

## Model 1: title as input only

Since the output of fake will be either 1 or 0, we add one more Dense layer on title_features.


```python
output1 = layers.Dense(2, name = "fake")(title_features)
```

Create the model1 that has title_input for inputs, and output1 for outputs.


```python
model1 = keras.Model(
    inputs = title_input,
    outputs = output1
)
```


```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 2)            4000      
                                                                     
     dropout (Dropout)           (None, 500, 2)            0         
                                                                     
     global_average_pooling1d (G  (None, 2)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 2)                 0         
                                                                     
     dense (Dense)               (None, 32)                96        
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 4,162
    Trainable params: 4,162
    Non-trainable params: 0
    _________________________________________________________________



```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train model 1, and review its validation accuracy.


```python
history1 = model1.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 9ms/step - loss: 0.6923 - accuracy: 0.5212 - val_loss: 0.6911 - val_accuracy: 0.5293
    Epoch 2/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.6906 - accuracy: 0.5248 - val_loss: 0.6875 - val_accuracy: 0.5311
    Epoch 3/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.6746 - accuracy: 0.6127 - val_loss: 0.6496 - val_accuracy: 0.9198
    Epoch 4/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.5965 - accuracy: 0.8146 - val_loss: 0.5282 - val_accuracy: 0.9204
    Epoch 5/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.4556 - accuracy: 0.8920 - val_loss: 0.3656 - val_accuracy: 0.9407
    Epoch 6/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.3349 - accuracy: 0.9154 - val_loss: 0.2628 - val_accuracy: 0.9491
    Epoch 7/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.2637 - accuracy: 0.9223 - val_loss: 0.1922 - val_accuracy: 0.9611
    Epoch 8/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.2162 - accuracy: 0.9321 - val_loss: 0.1605 - val_accuracy: 0.9644
    Epoch 9/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.1861 - accuracy: 0.9400 - val_loss: 0.1331 - val_accuracy: 0.9651
    Epoch 10/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1659 - accuracy: 0.9421 - val_loss: 0.1139 - val_accuracy: 0.9747
    Epoch 11/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1529 - accuracy: 0.9436 - val_loss: 0.0959 - val_accuracy: 0.9756
    Epoch 12/30
    180/180 [==============================] - 1s 8ms/step - loss: 0.1400 - accuracy: 0.9464 - val_loss: 0.0878 - val_accuracy: 0.9773
    Epoch 13/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1316 - accuracy: 0.9473 - val_loss: 0.0859 - val_accuracy: 0.9733
    Epoch 14/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.1262 - accuracy: 0.9515 - val_loss: 0.0691 - val_accuracy: 0.9800
    Epoch 15/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1184 - accuracy: 0.9531 - val_loss: 0.0667 - val_accuracy: 0.9778
    Epoch 16/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.1149 - accuracy: 0.9523 - val_loss: 0.0708 - val_accuracy: 0.9827
    Epoch 17/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.1106 - accuracy: 0.9529 - val_loss: 0.0604 - val_accuracy: 0.9833
    Epoch 18/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.1049 - accuracy: 0.9564 - val_loss: 0.0598 - val_accuracy: 0.9811
    Epoch 19/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.1004 - accuracy: 0.9578 - val_loss: 0.0577 - val_accuracy: 0.9820
    Epoch 20/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0988 - accuracy: 0.9583 - val_loss: 0.0526 - val_accuracy: 0.9849
    Epoch 21/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0972 - accuracy: 0.9582 - val_loss: 0.0651 - val_accuracy: 0.9809
    Epoch 22/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0964 - accuracy: 0.9587 - val_loss: 0.0502 - val_accuracy: 0.9838
    Epoch 23/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0950 - accuracy: 0.9585 - val_loss: 0.0504 - val_accuracy: 0.9831
    Epoch 24/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0922 - accuracy: 0.9597 - val_loss: 0.0404 - val_accuracy: 0.9880
    Epoch 25/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0895 - accuracy: 0.9608 - val_loss: 0.0490 - val_accuracy: 0.9858
    Epoch 26/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0911 - accuracy: 0.9585 - val_loss: 0.0432 - val_accuracy: 0.9880
    Epoch 27/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0880 - accuracy: 0.9594 - val_loss: 0.0427 - val_accuracy: 0.9863
    Epoch 28/30
    180/180 [==============================] - 2s 8ms/step - loss: 0.0871 - accuracy: 0.9591 - val_loss: 0.0427 - val_accuracy: 0.9867
    Epoch 29/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0862 - accuracy: 0.9585 - val_loss: 0.0397 - val_accuracy: 0.9880
    Epoch 30/30
    180/180 [==============================] - 1s 7ms/step - loss: 0.0849 - accuracy: 0.9608 - val_loss: 0.0498 - val_accuracy: 0.9818


To easier view the accuracy at each epoch, we may made a plot on it.


```python
plt.plot(history1.history["accuracy"], label = "training")
plt.plot(history1.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa679868ed0>




    
![png](output_31_1.png)
    


From the plot, we may conclude that the model1 is not overfitting. Based on history1, the validating accuracy is about 98%.

## Model 2: text as input only

Also add one more Dense layer on text_features.


```python
output2 = layers.Dense(2, name = "fake")(text_features)
```

Create a model2 that has text_input for inputs, and output2 for outputs.


```python
model2 = keras.Model(
    inputs = text_input,
    outputs = output2
)
```


```python
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 2)            4000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 2)            0         
                                                                     
     global_average_pooling1d_1   (None, 2)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 2)                 0         
                                                                     
     dense_1 (Dense)             (None, 32)                96        
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 4,162
    Trainable params: 4,162
    Non-trainable params: 0
    _________________________________________________________________



```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train the data and review the history2.


```python
history2 = model2.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 4s 15ms/step - loss: 0.6467 - accuracy: 0.6384 - val_loss: 0.5831 - val_accuracy: 0.7644
    Epoch 2/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.5065 - accuracy: 0.7975 - val_loss: 0.3977 - val_accuracy: 0.8865
    Epoch 3/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.3618 - accuracy: 0.8695 - val_loss: 0.2832 - val_accuracy: 0.9036
    Epoch 4/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.2879 - accuracy: 0.8974 - val_loss: 0.2474 - val_accuracy: 0.9238
    Epoch 5/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.2609 - accuracy: 0.9061 - val_loss: 0.2043 - val_accuracy: 0.9409
    Epoch 6/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.2363 - accuracy: 0.9183 - val_loss: 0.1948 - val_accuracy: 0.9436
    Epoch 7/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.2252 - accuracy: 0.9234 - val_loss: 0.1795 - val_accuracy: 0.9511
    Epoch 8/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.2145 - accuracy: 0.9255 - val_loss: 0.1752 - val_accuracy: 0.9491
    Epoch 9/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.2035 - accuracy: 0.9305 - val_loss: 0.1659 - val_accuracy: 0.9504
    Epoch 10/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1974 - accuracy: 0.9312 - val_loss: 0.1627 - val_accuracy: 0.9551
    Epoch 11/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1957 - accuracy: 0.9310 - val_loss: 0.1480 - val_accuracy: 0.9576
    Epoch 12/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1826 - accuracy: 0.9385 - val_loss: 0.1442 - val_accuracy: 0.9611
    Epoch 13/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1774 - accuracy: 0.9408 - val_loss: 0.1359 - val_accuracy: 0.9609
    Epoch 14/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1715 - accuracy: 0.9420 - val_loss: 0.1233 - val_accuracy: 0.9693
    Epoch 15/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1665 - accuracy: 0.9404 - val_loss: 0.1344 - val_accuracy: 0.9653
    Epoch 16/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1631 - accuracy: 0.9430 - val_loss: 0.1203 - val_accuracy: 0.9684
    Epoch 17/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1593 - accuracy: 0.9436 - val_loss: 0.1322 - val_accuracy: 0.9660
    Epoch 18/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1504 - accuracy: 0.9455 - val_loss: 0.1189 - val_accuracy: 0.9672
    Epoch 19/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.1522 - accuracy: 0.9461 - val_loss: 0.1190 - val_accuracy: 0.9701
    Epoch 20/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1517 - accuracy: 0.9470 - val_loss: 0.1101 - val_accuracy: 0.9696
    Epoch 21/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1480 - accuracy: 0.9469 - val_loss: 0.1066 - val_accuracy: 0.9718
    Epoch 22/30
    180/180 [==============================] - 3s 16ms/step - loss: 0.1403 - accuracy: 0.9505 - val_loss: 0.1159 - val_accuracy: 0.9669
    Epoch 23/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1419 - accuracy: 0.9497 - val_loss: 0.1054 - val_accuracy: 0.9727
    Epoch 24/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1373 - accuracy: 0.9512 - val_loss: 0.0974 - val_accuracy: 0.9740
    Epoch 25/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1343 - accuracy: 0.9516 - val_loss: 0.1069 - val_accuracy: 0.9704
    Epoch 26/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1373 - accuracy: 0.9489 - val_loss: 0.0938 - val_accuracy: 0.9747
    Epoch 27/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1290 - accuracy: 0.9540 - val_loss: 0.0832 - val_accuracy: 0.9791
    Epoch 28/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1292 - accuracy: 0.9533 - val_loss: 0.0947 - val_accuracy: 0.9740
    Epoch 29/30
    180/180 [==============================] - 3s 15ms/step - loss: 0.1229 - accuracy: 0.9544 - val_loss: 0.0964 - val_accuracy: 0.9744
    Epoch 30/30
    180/180 [==============================] - 3s 14ms/step - loss: 0.1248 - accuracy: 0.9540 - val_loss: 0.0856 - val_accuracy: 0.9739


To easily view the accuracy at each epoch, we may made a plot on it.


```python
plt.plot(history2.history["accuracy"], label = "training")
plt.plot(history2.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa6792e0a50>




    
![png](output_42_1.png)
    


From the plot, model2 is not overfitting as well. And base on the history2, the vilidating accuracy is about 97%.

## Model 3: title and text as input

Since we have two distinct inputs, we need to concatenate them and them apply two more Dense layer on them.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```

Create a model3 that has title_input, and text_input for inputs and output for outputs.


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```


```python
model3.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['title[0][0]',                  
     ization)                                                         'text[0][0]']                   
                                                                                                      
     embedding (Embedding)          (None, 500, 2)       4000        ['text_vectorization[0][0]',     
                                                                      'text_vectorization[1][0]']     
                                                                                                      
     dropout (Dropout)              (None, 500, 2)       0           ['embedding[0][0]']              
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 2)       0           ['embedding[1][0]']              
                                                                                                      
     global_average_pooling1d (Glob  (None, 2)           0           ['dropout[0][0]']                
     alAveragePooling1D)                                                                              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 2)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_1 (Dropout)            (None, 2)            0           ['global_average_pooling1d[0][0]'
                                                                     ]                                
                                                                                                      
     dropout_3 (Dropout)            (None, 2)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dense (Dense)                  (None, 32)           96          ['dropout_1[0][0]']              
                                                                                                      
     dense_1 (Dense)                (None, 32)           96          ['dropout_3[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense[0][0]',                  
                                                                      'dense_1[0][0]']                
                                                                                                      
     dense_2 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_2[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 6,338
    Trainable params: 6,338
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Train the model3 and review history3.


```python
history3 = model3.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = True)
```

    Epoch 1/30
    180/180 [==============================] - 5s 20ms/step - loss: 0.1577 - accuracy: 0.9785 - val_loss: 0.0471 - val_accuracy: 0.9863
    Epoch 2/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0491 - accuracy: 0.9855 - val_loss: 0.0330 - val_accuracy: 0.9892
    Epoch 3/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0408 - accuracy: 0.9862 - val_loss: 0.0237 - val_accuracy: 0.9924
    Epoch 4/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0375 - accuracy: 0.9864 - val_loss: 0.0218 - val_accuracy: 0.9916
    Epoch 5/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0369 - accuracy: 0.9877 - val_loss: 0.0192 - val_accuracy: 0.9935
    Epoch 6/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0361 - accuracy: 0.9870 - val_loss: 0.0186 - val_accuracy: 0.9929
    Epoch 7/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 0.0173 - val_accuracy: 0.9938
    Epoch 8/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 0.0231 - val_accuracy: 0.9927
    Epoch 9/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0326 - accuracy: 0.9885 - val_loss: 0.0140 - val_accuracy: 0.9958
    Epoch 10/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0318 - accuracy: 0.9889 - val_loss: 0.0141 - val_accuracy: 0.9947
    Epoch 11/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0322 - accuracy: 0.9887 - val_loss: 0.0153 - val_accuracy: 0.9947
    Epoch 12/30
    180/180 [==============================] - 4s 24ms/step - loss: 0.0340 - accuracy: 0.9885 - val_loss: 0.0166 - val_accuracy: 0.9947
    Epoch 13/30
    180/180 [==============================] - 5s 28ms/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 0.0122 - val_accuracy: 0.9960
    Epoch 14/30
    180/180 [==============================] - 4s 24ms/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 0.0140 - val_accuracy: 0.9951
    Epoch 15/30
    180/180 [==============================] - 5s 29ms/step - loss: 0.0309 - accuracy: 0.9896 - val_loss: 0.0232 - val_accuracy: 0.9909
    Epoch 16/30
    180/180 [==============================] - 5s 26ms/step - loss: 0.0271 - accuracy: 0.9898 - val_loss: 0.0165 - val_accuracy: 0.9949
    Epoch 17/30
    180/180 [==============================] - 6s 32ms/step - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.0107 - val_accuracy: 0.9962
    Epoch 18/30
    180/180 [==============================] - 4s 24ms/step - loss: 0.0279 - accuracy: 0.9901 - val_loss: 0.0167 - val_accuracy: 0.9944
    Epoch 19/30
    180/180 [==============================] - 5s 28ms/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: 0.0098 - val_accuracy: 0.9969
    Epoch 20/30
    180/180 [==============================] - 5s 29ms/step - loss: 0.0299 - accuracy: 0.9894 - val_loss: 0.0109 - val_accuracy: 0.9967
    Epoch 21/30
    180/180 [==============================] - 5s 30ms/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.0112 - val_accuracy: 0.9953
    Epoch 22/30
    180/180 [==============================] - 4s 19ms/step - loss: 0.0282 - accuracy: 0.9896 - val_loss: 0.0091 - val_accuracy: 0.9971
    Epoch 23/30
    180/180 [==============================] - 5s 29ms/step - loss: 0.0243 - accuracy: 0.9916 - val_loss: 0.0101 - val_accuracy: 0.9967
    Epoch 24/30
    180/180 [==============================] - 4s 22ms/step - loss: 0.0251 - accuracy: 0.9911 - val_loss: 0.0116 - val_accuracy: 0.9956
    Epoch 25/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 0.0089 - val_accuracy: 0.9980
    Epoch 26/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0072 - val_accuracy: 0.9984
    Epoch 27/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.0083 - val_accuracy: 0.9978
    Epoch 28/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0262 - accuracy: 0.9909 - val_loss: 0.0084 - val_accuracy: 0.9978
    Epoch 29/30
    180/180 [==============================] - 3s 18ms/step - loss: 0.0263 - accuracy: 0.9907 - val_loss: 0.0058 - val_accuracy: 0.9984
    Epoch 30/30
    180/180 [==============================] - 3s 19ms/step - loss: 0.0247 - accuracy: 0.9910 - val_loss: 0.0102 - val_accuracy: 0.9960


Made a plot to see how accuracy changes at each epoch.


```python
plt.plot(history3.history["accuracy"], label = "training")
plt.plot(history3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fa678f51fd0>




    
![png](output_54_1.png)
    


From the plot, the traning line is below the validation line, which indicates the model3 is not overfitting. And the history3 shows that the accuracy is about 99.7%, which is barely close to 100%.

Comparing these three models, we can see that the model3 performs better, since it has accuracy close to 100%.

# 4. Model Evaluation

Download the following url for testing data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```


```python
test_df = pd.read_csv(test_url)
```

View on test_df.


```python
test_df
```





  <div id="df-79bf375c-81bd-4f42-8814-cb6ab397f70a">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420</td>
      <td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
      <td>Donald Trump practically does something to cri...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14902</td>
      <td>Exclusive: Kremlin tells companies to deliver ...</td>
      <td>The Kremlin wants good news.  The Russian lead...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>322</td>
      <td>Golden State Warriors Coach Just WRECKED Trum...</td>
      <td>On Saturday, the man we re forced to call  Pre...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16108</td>
      <td>Putin opens monument to Stalin's victims, diss...</td>
      <td>President Vladimir Putin inaugurated a monumen...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10304</td>
      <td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
      <td>Apparently breaking the law and scamming the g...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>20058</td>
      <td>U.S. will stand be steadfast ally to Britain a...</td>
      <td>The United States will stand by Britain as it ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>21104</td>
      <td>Trump rebukes South Korea after North Korean b...</td>
      <td>U.S. President Donald Trump admonished South K...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>2842</td>
      <td>New rule requires U.S. banks to allow consumer...</td>
      <td>U.S. banks and credit card companies could be ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>22298</td>
      <td>US Middle Class Still Suffering from Rockefell...</td>
      <td>Dick Eastman The Truth HoundWhen Henry Kissin...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>333</td>
      <td>Scaramucci TV Appearance Goes Off The Rails A...</td>
      <td>The most infamous characters from Donald Trump...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-79bf375c-81bd-4f42-8814-cb6ab397f70a')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-79bf375c-81bd-4f42-8814-cb6ab397f70a button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-79bf375c-81bd-4f42-8814-cb6ab397f70a');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




Call make_dataset on test_df to modify title and text columns.


```python
test_df = make_dataset(test_df)
```

Apply the best model3 on test_df.


```python
model3.evaluate(test_df)
```

    225/225 [==============================] - 2s 10ms/step - loss: 0.0266 - accuracy: 0.9914





    [0.0265724565833807, 0.9914472699165344]



Based on the model3 on testing data result, we have 99% of confidenct that we'll be right on predict fake news.

# 5. Embedding

Visualize on the embedding that our best model learned.


```python
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()   
```


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

embedding_df
```





  <div id="df-4efcfcef-f923-4f50-bf83-648f2dc156a6">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>0.236652</td>
      <td>-0.007172</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.098647</td>
      <td>-0.018294</td>
    </tr>
    <tr>
      <th>2</th>
      <td>trump</td>
      <td>0.096833</td>
      <td>-0.089702</td>
    </tr>
    <tr>
      <th>3</th>
      <td>to</td>
      <td>-4.987768</td>
      <td>0.027331</td>
    </tr>
    <tr>
      <th>4</th>
      <td>video</td>
      <td>-5.694953</td>
      <td>-0.134849</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>sues</td>
      <td>0.358802</td>
      <td>-0.003899</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>sued</td>
      <td>1.136319</td>
      <td>-0.041800</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>strategist</td>
      <td>0.519704</td>
      <td>-0.087125</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>stephen</td>
      <td>-0.753656</td>
      <td>-0.021906</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>spoke</td>
      <td>0.196462</td>
      <td>-0.043374</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-4efcfcef-f923-4f50-bf83-648f2dc156a6')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-4efcfcef-f923-4f50-bf83-648f2dc156a6 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-4efcfcef-f923-4f50-bf83-648f2dc156a6');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




Use plot to visualize how words located.


```python
import plotly.express as px 
import numpy as np
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))*2),
                 size_max = 4,
                 hover_name = "word")

fig.show()
```


<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="77463ad9-2649-4b0c-83ae-1ea396c953dc" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("77463ad9-2649-4b0c-83ae-1ea396c953dc")) {                    Plotly.newPlot(                        "77463ad9-2649-4b0c-83ae-1ea396c953dc",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<extra></extra>","hovertext":["","[UNK]","trump","to","video","the","us","for","of","in","says","on","a","and","is","obama","with","watch","house","hillary","after","about","his","new","clinton","white","trump\u2019s","president","just","at","by","from","bill","state","russia","this","out","who","he","it","republican","court","north","over","senate","her","as","are","will","election","him","donald","calls","black","news","how","not","media","be","vote","breaking","korea","why","that","republicans","police","gop","you","\u2013","was","muslim","tax","campaign","may","trumps","has","one","deal","up","obama\u2019s","democrats","china","down","iran","gets","what","tweets","government","party","former","fbi","back","attack","talks","russian","people","against","un","have","congress","syria","security","top","chief","first","eu","cnn","judge","they","senator","pm","america","no","speech","plan","fox","ban","war","leader","their","tells","democrat","law","twitter","say","would","man","brexit","report","could","minister","during","makes","it\u2019s","million","military","make","sanders","more","presidential","when","south","she","shows","probe","factbox","racist","like","liberal","goes","official","get","two","tweet","take","nuclear","border","supreme","if","hillary\u2019s","being","woman","foreign","governor","sanctions","into","healthcare","political","gun","american","poll","time","syrian","rally","obamacare","support","off","cruz","americans","all","wow","an","show","illegal","host","fight","want","them","go","islamic","day","putin","wants","supporters","uk","rights","response","race","budget","bernie","attacks","national","german","ryan","urges","next","meet","he\u2019s","claims","because","lawmakers","help","meeting","crisis","candidate","call","women","saudi","refugees","mexico","details","debate","antitrump","your","school","policy","students","states","we","turkey","trade","our","must","group","asks","stop","leaders","don\u2019t","conservative","killed","defense","world","win","fake","while","old","warns","visit","sources","see","hilarious","gives","won\u2019t","way","takes","press","panel","most","got","comey","can","room","opposition","officials","immigration","big","years","travel","texas","administration","so","left","general","email","department","democratic","death","aid","than","should","secretary","move","cops","city","tillerson","ted","plans","huge","climate","caught","air","still","seeks","right","mayor","here\u2019s","going","before","wall","major","head","did","voters","ties","emails","pick","john","free","end","reporter","case","rules","open","money","leftist","justice","year","washington","truth","protesters","lives","arrested","supporter","reason","federal","shocking","paul","health","conservatives","secret","reform","presidency","violence","list","iraq","i","but","live","give","really","push","kill","exclusive","can\u2019t","attorney","use","myanmar","made","interview","do","director","dead","speaker","last","george","need","jerusalem","investigation","college","change","work","team","family","order","only","key","home","britain","tv","nominee","merkel","dnc","boiler","billion","social","run","lawyer","voter","told","sexual","puerto","lie","business","ahead","york","lol","job","image","face","threatens","said","post","pence","peace","now","legal","lawmaker","threat","rohingya","latest","high","forces","fire","ep","control","bid","times","terrorist","rico","protest","office","mccain","japan","cut","british","shooting","ever","decision","again","never","keep","germany","boom","trying","bomb","amid","public","message","macron","know","isis","funding","force","even","doesn\u2019t","discuss","destroys","army","adviser","2016","used","statement","release","refugee","or","lies","korean","iraqi","busted","votes","senators","ready","own","kremlin","fired","cuba","using","terrorists","terror","story","source","israel","france","florida","asked","another","slams","radical","intelligence","independence","had","flag","coalition","charges","blasts","behind","ruling","power","needs","jobs","himself","arrest","msnbc","missile","matter","march","california","best","backs","agency","week","violent","tries","parliament","nfl","lead","hate","chicago","chair","called","bad","ad","\u201ci","were","sean","nyc","didn\u2019t","catalan","bush","benghazi","\u201cthe","muslims","great","full","congressman","young","voting","sign","shut","set","seek","protests","likely","french","aide","admits","under","threats","tell","special","possible","orders","kills","flynn","despite","defends","cia","bombshell","been","yet","scandal","refuses","near","london","internet","inauguration","hollywood","hit","girl","chinas","3","pay","making","cop","clinton\u2019s","believe","turkish","son","saying","rep","found","does","action","vows","very","shot","sees","reports","real","kids","hurricane","good","days","comments","assault","sex","rejects","lying","away","wife","victims","united","three","thing","these","sessions","rule","reveals","repeal","proves","let","kurdish","hold","hack","final","epic","disgusting","destroy","dem","corruption","claim","awesome","whoa","trip","think","rubio","history","groups","elections","10","warren","street","service","second","sarah","released","questions","pope","middle","ivanka","issues","images","forced","facebook","cuts","biden","audio","wins","transgender","start","senior","seen","russias","review","rape","pressure","planned","pentagon","passes","much","moore","michelle","jail","hell","fraud","executive","evidence","every","east","citizens","children","announces","act","spokesman","running","photo","mike","men","joe","hard","erdogan","crooked","committee","chris","brutal","ambassador","al","water","victory","venezuela","trial","stunning","spending","rips","rant","program","nato","least","hilariously","future","epa","effort","doj","country","abortion","\u201cwe","uses","strike","star","role","protect","pelosi","king","gave","flashback","five","barack","approves","agree","5","1","vp","she\u2019s","put","ny","mcconnell","look","little","liberals","immigrants","hits","drops","debt","criminal","convention","break","boy","blames","ben","battle","xi","working","workers","viral","question","offers","offer","navy","massive","jr","finally","envoy","dc","cnn\u2019s","activists","activist","911","wikileaks","union","summit","schools","releases","part","members","kellyanne","fans","exposes","crowd","conference","calling","yemen","university","too","strikes","oil","nancy","letter","leave","kelly","illegals","hearing","getting","four","fed","denies","demands","child","challenge","ceo","arms","arabia","angry","agenda","4","words","town","they\u2019re","terrorism","target","suspected","since","return","other","oops","loses","dangerous","company","church","catalonia","canada","cabinet","alien","actually","2018","without","transition","taking","puts","nafta","murder","michael","life","killing","join","issue","hopes","global","friend","friday","find","female","exposed","defend","cyber","come","close","car","asia","anchor","weapons","warning","troops","thinks","student","stage","spicer","signs","remarks","perfect","oregon","millions","militants","michigan","member","line","leaves","laws","hacking","food","fear","faces","conway","brilliant","aliens","accuses","2","tried","test","some","picture","picks","philippines","ohio","night","mocks","manager","labor","fund","energy","bank","announcement","yr","wrong","west","ukraine","turn","talk","step","stand","spain","soros","ria","request","politics","megyn","johnson","irma","gay","fighting","fck","elizabeth","civil","blame","around","already","thousands","steps","staff","speaks","responds","resign","private","percent","paris","mueller","guy","guns","funds","far","deputy","confirms","breaks","block","better","afghan","system","points","palin","number","mexican","mattis","james","finance","economy","detroit","destroyed","dems","daughter","congressional","christmas","chairman","ask","alabama","accused","vietnam","telling","sunday","steve","speak","something","revealed","professor","names","mom","migrants","mass","lebanon","israeli","immigrant","hannity","guest","foundation","eyes","everyone","employees","crazy","coming","coal","care","bundy","appeals","20","100","\u2018the","where","voted","videos","tough","sue","stay","sht","sheriff","sent","same","rich","referendum","reelection","moscow","melania","lose","intel","illinois","held","heads","happened","frances","flint","firm","embarrassing","economic","council","carson","carolina","boost","bathroom","actor","waters","update","super","sends","roy","racism","perfectly","pastor","pass","parties","its","irish","iowa","human","europe","ends","efforts","cities","chinese","charged","charge","changes","capital","candidates","baltimore","antifa","agencies","access","15","zimbabwe","worst","well","uks","turkeys","tucker","things","reporters","record","prison","prince","powerful","pakistan","northern","name","mugabe","maxine","market","less","kurds","insane","giving","explains","embassy","drug","demand","class","choice","bangladesh","allow","allies","virginia","urge","tuesday","train","then","syrias","sets","save","russians","reutersipsos","parenthood","my","movie","lost","looks","launches","journalist","ireland","germanys","financial","event","due","disaster","dad","corporate","conspiracy","carlson","become","bans","any","allegations","african","afghanistan","abuse","unhinged","try","totally","today","streets","sea","screenshots","residents","quit","qatar","moment","meltdown","mark","literally","lawsuit","launch","jeanine","jailed","humiliates","highlights","hariri","girls","game","furious","finds","doing","desperate","comment","comes","brutally","boycott","blow","between","asking","armed","anthem","answer","wearing","throws","taxes","targets","sick","sanctuary","raise","prosecutor","play","palestinian","pact","owner","might","journalists","important","henningsen","fires","eus","documents","criticism","britains","bring","australia","arrests","absolutely","8","13","zimbabwes","worse","turns","threatened","third","testimony","term","teen","teacher","soldiers","send","romney","resigns","proof","prime","praises","place","o\u2019reilly","officer","obamas","months","missing","meets","hurt","hot","european","ethics","endorses","egypt","drop","donations","detained","declares","community","communist","borders","attacked","america\u2019s","winning","who\u2019s","victim","there","strong","storm","soon","socialist","shuts","results","replace","reaction","raises","promises","position","players","parents","opens","nomination","netanyahu","nations","nation","moves","many","kim","iranian","industry","hosts","homeland","done","confederate","condemns","christie","christian","book","appears","amnesty","aides","agents","across","12","11","you\u2019re","women\u2019s","vs","thugs","taiwan","supports","supporting","spy","spanish","seven","seth","rnc","remove","religious","regime","red","rebels","progress","mother","month","love","living","legislation","leads","information","hysterical","hope","he\u2019ll","guilty","farright","education","disturbing","diplomatic","december","continues","companies","board","base","attend","asylum","assad","50","veteran","treatment","thursday","testify","tensions","talking","study","six","sentence","returns","problem","popular","philippine","overhaul","nothing","nbc","nails","long","joins","inside","hundreds","hand","guess","front","freedom","facts","elected","delivers","data","dallas","crime","counsel","continue","baby","avoid","airport","ads","7","word","waiting","visits","vice","veterans","trump\u201d","total","taxpayer","suggests","schumer","sales","rightwing","read","reach","quits","pushes","polls","person","patrick","outside","mosque","lied","libya","leaked","leading","lavrov","kushner","koreas","jeff","japans","injured","hispanic","hateful","harvey","harassment","hands","firing","feds","experts","exit","electoral","early","draws","dollars","dispute","die","dept","crimes","credit","crackdown","cooperation","concerns","concerned","complete","chaos","central","center","cash","campus","blacklivesmatter","australian","agent","17","\u201cif","wisconsin","watchdog","warned","wanted","thought","taxpayers","sec","scott","relief","relations","protester","potential","posts","orlando","nyt","newspaper","nearly","militant","manafort","kerry","kenya","international","idea","hypocrite","humiliated","half","hacked","ground","green","gov","golf","focus","father","families","enough","eastern","delay","cover","clintons","check","charlottesville","cannot","broke","billionaire","bannon","banks","attempt","anyone","agrees","address","9","6","2017","18","\u201cyou","\u201cracist\u201d","you\u2019ll","worker","went","wage","w","visa","van","usa","unity","thug","threatening","thanks","suicide","starts","spying","small","scam","robert","risk","reid","recount","plot","news\u2019","murdered","marriage","lynch","loss","lady","joy","jimmy","india","hypocrisy","husband","harry","gas","fix","files","fail","expose","explain","emergency","defeat","cites","chuck","camp","ca","buy","blocks","biggest","backing","attempts","attacking","ally","alleged","advisor","abc","vegas","trumprussia","took","sweden","suspect","susan","surprise","situation","shutdown","road","riots","respond","relationship","region","proposes","process","past","options","once","nra","nightmare","militia","migrant","met","maher","low","libyan","leaks","leadership","late","kkk","jeb","irs","info","hotel","here","having","happen","halt","gowdy","fit","fan","failed","eric","drive","dialogue","deep","decide","cost","completely","clash","charity","brother","brings","body","arizona","appeal","admit","actions","30","\u201cthis","\u201che","\u2014","whining","weighs","undercover","trumpcare","trey","treasury","threaten","that\u2019s","taken","stupid","strategy","station","speaking","someone","somali","seeking","rush","rise","ridiculous","removed","remain","reasons","reality","propaganda","point","pledges","pathetic","paid","outrageous","others","officers","mi","looms","leaving","keeps","kasich","islamist","irans","instead","hunt","hospital","holds","helping","hammers","gonna","gingrich","forward","false","expects","expected","endorsement","dossier","david","countries","conflict","committed","classified","citizen","christians","chelsea","bus","blaming","banned","audience","april","apart","accept","abe","\u201cnot","\u201cit\u2019s","unreal","tim","tests","swedish","straight","stealing","showing","shooter","sexist","sell","rice","reuters","responsible","pulls","price","politico","policies","pledge","path","nuts","numbers","network","nazi","mnuchin","mays","longer","isn\u2019t","investment","insurance","feel","fears","famous","exactly","everything","duterte","diplomats","dies","detains","defending","deals","deadly","criticizes","commission","commander","collusion","breitbart","blast","blacks","bizarre","amazing","allowed","ago","africas","25","won","whines","which","whether","weeks","trudeau","truck","torture","toll","through","tech","suspends","stance","spent","serious","scalia","safe","resolve","reforms","ratings","prove","priceless","plane","phone","outrage","opening","nsa","myanmars","ministry","memorial","meddling","me","lee","lawyers","kimmel","jersey","it\u201d","huckabee","holding","hiding","helped","hear","gorsuch","glorious","fuel","friends","falls","expert","entire","endorse","door","donors","courts","controversial","congo","closed","chance","burn","briefing","bombers","blood","bills","aren\u2019t","america\u201d","amendment","accusations","accidentally","24","worked","we\u2019re","welfare","view","unlikely","tpp","those","there\u2019s","teachers","shouldn\u2019t","sharpton","resignation","regional","raqqa","radio","protrump","project","probes","primary","presidents","poor","playing","photos","phony","opposes","n","movement","monday","minimum","memo","marco","majority","lot","local","kidding","kicked","jones","joint","jackson","islam","infrastructure","influence","increase","incident","hours","historic","hezbollah","guard","graft","given","giuliani","floor","failure","easy","drone","domestic","dinner","declaration","dark","cuomo","critical","couple","corrupt","commerce","cheer","cbs","caucus","cases","career","brussels","brazils","brazil","bombing","beijing","beaten","aims","aim","actress","account","\u201cwhite","\u201ci\u2019m","wh","wealthy","vet","trust","trillion","tape","tantrum","standing","snl","single","silence","shreds","sharia","search","scheme","san","records","rand","problems","privacy","planning","pipeline","personal","owned","online","oklahoma","nominees","nigeria","newt","news\u201d","mock","linked","limits","kurdistan","killer","kansas","june","judges","japanese","italy","interference","indian","hitler","google","gone","gold","gift","flight","flags","firms","fellow","facing","disabled","coup","convicted","considering","confident","cold","coast","canadian","camera","burns","berkeley","becoming","beautiful","beating","beat","ballot","baghdad","arab","allowing","agreement","witch","willing","welcomes","wednesday","website","wasn\u2019t","va","usbacked","turned","tower","toward","tom","todd","themselves","systems","swiss","sudan","stuns","sports","spokeswoman","speed","sold","several","scotus","runs","ross","reportedly","remember","queen","putting","proposal","promote","poland","pennsylvania","peninsula","paying","neil","nasty","multiple","mn","mind","medical","massacre","links","lets","legacy","las","jay","i\u2019m","iraqs","indonesian","independent","incredible","including","happens","haley","graham","governors","georgia","gender","funeral","feud","fall","fails","establishment","environmental","doubt","dollar","deaths","cuban","critics","create","crash","coverage","congresswoman","compares","comey\u2019s","claiming","cancer","cancels","canadas","building","boss","blocking","blocked","birthday","believes","becomes","bar","approve","accuse","21","2015","14","\u201ca","\u2018fake","zealand","worried","western","warming","vatican","until","true","trolls","tears","targeted","sure","sues","sued","strategist","stephen","spoke"],"legendgroup":"","marker":{"color":"#636efa","size":[2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0],"sizemode":"area","sizeref":0.125,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[0.23665237426757812,0.09864723682403564,0.09683261066675186,-4.987767696380615,-5.694953441619873,0.35164061188697815,1.5185327529907227,-3.8060238361358643,-4.842273235321045,-1.0822581052780151,1.4691791534423828,-2.3194737434387207,0.26955506205558777,-3.540578603744507,-4.291755199432373,-1.4679163694381714,-2.5746402740478516,-4.7491068840026855,1.4396711587905884,-3.196506977081299,-3.1140694618225098,-2.246210813522339,-1.829848051071167,0.25161662697792053,0.4866617023944855,0.5023360848426819,-4.738358020782471,0.03666972741484642,-5.142655372619629,-2.3498246669769287,-4.54532527923584,-3.765651226043701,0.4145316481590271,0.40758830308914185,-0.14538323879241943,-2.7148382663726807,-1.6157418489456177,-3.568187952041626,-0.9037020206451416,-1.1912837028503418,2.0210630893707275,1.1869474649429321,0.37834471464157104,-3.0215935707092285,1.449630618095398,-1.1567946672439575,-2.1200544834136963,-3.523634910583496,-3.1392745971679688,2.0043540000915527,-0.913479745388031,-0.2181503176689148,1.2907721996307373,-0.8169617652893066,-1.003908634185791,-2.917912721633911,-2.6809160709381104,0.3359883427619934,-2.3882639408111572,0.8198747038841248,-4.9563679695129395,0.5595536231994629,-2.9351134300231934,-2.321869134902954,-0.2545938491821289,1.0574498176574707,-7.38219690322876,-2.043323278427124,-1.131991982460022,-3.626870632171631,-2.2738215923309326,1.851576566696167,1.1824264526367188,0.5193281769752502,8.753466606140137,-3.4627342224121094,0.17547564208507538,1.1978473663330078,-1.9484429359436035,-4.063047885894775,0.5185614824295044,2.4667112827301025,-1.856832504272461,-0.08441518992185593,-1.7346270084381104,-1.8494606018066406,-0.6670590043067932,0.9077969193458557,0.1762380599975586,1.2190643548965454,-1.427921175956726,-0.39467963576316833,-0.006479381583631039,3.2919204235076904,-0.4755599796772003,1.0352293252944946,-3.444305419921875,1.026336431503296,-3.0314157009124756,0.8351420164108276,-0.6663301587104797,0.09818921238183975,0.5193594098091125,2.682896375656128,0.8503749370574951,2.129395008087158,-1.082463264465332,-0.375746488571167,-2.0618042945861816,1.0091913938522339,1.4284015893936157,-2.910099744796753,-0.5024812817573547,0.7123096585273743,0.09859463572502136,-1.8850880861282349,0.6125543713569641,-0.46555614471435547,0.8893237113952637,-2.4498908519744873,0.2998059391975403,0.1735849678516388,0.76482754945755,2.7926244735717773,1.8784360885620117,0.04597169905900955,-0.11192046105861664,2.75521183013916,-0.819503903388977,0.40973949432373047,2.431330680847168,-2.5131449699401855,-1.977587342262268,-2.361736536026001,0.9984708428382874,0.0003002142475452274,-0.15780192613601685,-0.45550116896629333,-4.645625591278076,3.336635112762451,-1.3329523801803589,2.445507287979126,-1.0177804231643677,-1.1564007997512817,3.311607599258423,5.155376434326172,-3.4107649326324463,-1.751236915588379,-2.8068888187408447,-2.867300510406494,0.8146044015884399,0.425586998462677,0.8405836224555969,0.2878355383872986,-0.3199271559715271,0.46958792209625244,1.059818983078003,-0.857837975025177,-1.4868099689483643,-3.1370973587036133,-2.07822322845459,-0.526378333568573,0.3767301142215729,0.9631079435348511,0.9678984880447388,-4.338040828704834,3.218801975250244,0.5828526020050049,-0.49853718280792236,-2.831596612930298,0.6987172961235046,-0.5894317626953125,0.6453412175178528,1.0375159978866577,0.6007172465324402,0.6771556735038757,-2.127882957458496,0.5576432347297668,-0.5607069134712219,-1.32417631149292,-2.676901340484619,0.12891845405101776,-0.026291096583008766,-1.6991688013076782,-1.9556446075439453,1.4758167266845703,0.06744765490293503,-0.05692244693636894,-0.772727906703949,2.462496280670166,-0.4380156993865967,0.7057565450668335,-0.5622048377990723,-0.5898278951644897,-0.488892525434494,1.3242560625076294,-0.5284873843193054,1.5319139957427979,1.7570182085037231,-2.479358673095703,1.365803599357605,0.34834492206573486,1.2342867851257324,0.14445827901363373,3.2676987648010254,1.0895969867706299,1.5590463876724243,-2.7335283756256104,-1.5350769758224487,-2.4738516807556152,1.6547247171401978,0.35423538088798523,0.10633460432291031,2.2830986976623535,1.2989219427108765,0.6510863304138184,1.3695573806762695,0.7572627663612366,-0.22957950830459595,1.1038898229599,-1.8013160228729248,1.6853985786437988,-1.357056736946106,-2.898771286010742,-0.6615761518478394,0.10618209093809128,-0.18696722388267517,1.3003841638565063,-0.26486337184906006,2.7512872219085693,2.6053566932678223,-2.2896628379821777,0.0244468841701746,1.0566513538360596,-0.04779919236898422,-0.017226044088602066,0.8817893862724304,-2.669774055480957,0.5075471997261047,3.1517446041107178,0.5054261088371277,-0.5620052218437195,0.24062059819698334,-0.15148569643497467,-1.0615193843841553,-0.9864375591278076,-0.9345739483833313,0.5726903080940247,0.9629629254341125,1.122137188911438,-2.9302124977111816,-0.6482038497924805,-2.8030283451080322,0.0013567776186391711,-0.30602872371673584,-2.1822056770324707,2.6606411933898926,-1.285553216934204,-1.6722025871276855,0.8304735422134399,-1.4475607872009277,-1.5445200204849243,3.390981674194336,0.9806635975837708,0.3721168041229248,0.12506826221942902,1.6949065923690796,0.5813699960708618,-0.2131308913230896,-0.33566349744796753,-1.6371994018554688,-1.8243321180343628,0.4016845226287842,0.4773840606212616,-0.5686830282211304,1.9955034255981445,0.721439778804779,0.9911909699440002,-2.8025829792022705,-1.7092951536178589,0.7018892765045166,-0.269010066986084,-3.7700088024139404,1.0674679279327393,0.8496907949447632,-1.8394818305969238,0.053810965269804,-2.9986813068389893,-0.11638451367616653,-2.9691059589385986,1.447567343711853,0.7107585072517395,3.354259729385376,-0.7037173509597778,-0.581490695476532,-2.187288761138916,-0.09328698366880417,-1.0112355947494507,-0.12798604369163513,0.014004118740558624,1.6425058841705322,-1.9948064088821411,0.7272393107414246,0.4025174379348755,-0.5222360491752625,1.2805496454238892,-2.4751453399658203,-0.4939957559108734,0.5669575333595276,-1.0118403434753418,0.8933227062225342,0.911323070526123,0.4717442989349365,-1.451453447341919,-2.787426233291626,0.9231650829315186,1.8460901975631714,-0.5156110525131226,-2.303607702255249,-0.35423508286476135,-1.1864093542099,0.4491010904312134,-1.6033165454864502,-1.8825385570526123,-0.5086442828178406,-4.370649337768555,-1.416961908340454,0.8601925373077393,-0.27583280205726624,-1.8607003688812256,1.5651514530181885,1.4118067026138306,0.20219579339027405,-1.404484510421753,1.1170612573623657,1.684944748878479,1.5692059993743896,-1.5028103590011597,-0.2092333287000656,-2.184703826904297,0.7130072116851807,-0.9750275015830994,0.6283794641494751,-2.5265727043151855,0.8609982132911682,0.6750285029411316,5.298497676849365,0.18828892707824707,1.202518105506897,-0.6167464852333069,0.7353705763816833,0.13947314023971558,1.2732653617858887,1.6193256378173828,-0.7469039559364319,-0.41758105158805847,0.46272191405296326,-0.4236014485359192,-0.38400769233703613,-0.39587321877479553,1.6540210247039795,-0.2657445967197418,0.5360988974571228,-0.04464808106422424,-1.9826478958129883,-0.4461711049079895,0.6032766699790955,2.759990692138672,2.4713547229766846,1.0104173421859741,1.2653459310531616,-2.6229989528656006,-4.981324672698975,0.9215191006660461,1.387175440788269,0.2475825548171997,0.11789318919181824,-0.9781748056411743,2.0816128253936768,-0.485684871673584,1.1751997470855713,-2.275468111038208,0.593278706073761,1.505739688873291,1.48704195022583,-2.4776716232299805,0.7448990345001221,-1.2594337463378906,0.37642717361450195,-1.0092663764953613,4.103524684906006,-0.3800167441368103,0.9601457118988037,0.8448963165283203,-1.6113148927688599,0.5292283892631531,2.647002696990967,-0.7750819325447083,4.722931861877441,-0.693749189376831,0.0456002876162529,1.7174025774002075,0.9349202513694763,-3.293929100036621,0.6210057139396667,2.6912336349487305,-0.6967445015907288,-1.7714585065841675,1.2550781965255737,1.1477978229522705,0.771564781665802,-0.555916428565979,3.109593629837036,1.0318876504898071,1.6463353633880615,-0.1482917070388794,-0.283312052488327,0.2966557443141937,-1.2698075771331787,0.09066437929868698,-0.1988905370235443,1.4751125574111938,-1.804917812347412,0.2715710699558258,1.7517940998077393,3.1419527530670166,0.1126570776104927,-0.32906827330589294,3.708549737930298,-0.17536777257919312,-4.1664533615112305,0.754616916179657,-1.1672240495681763,-2.708909511566162,-2.0992038249969482,1.3023542165756226,-1.790926218032837,0.9271584749221802,1.5367417335510254,-1.2706178426742554,0.7565203905105591,2.38216495513916,-0.06790142506361008,-0.5323023200035095,-3.535731077194214,-1.9532712697982788,0.6043342351913452,1.254814863204956,-3.284186840057373,0.4892890751361847,0.6744999289512634,0.7133697271347046,2.1794819831848145,2.6376166343688965,0.40973424911499023,1.9576598405838013,-1.1393935680389404,-3.5264205932617188,-3.0700719356536865,-1.238255500793457,2.7001094818115234,-0.08269146084785461,1.7580444812774658,0.48498469591140747,1.1394561529159546,-1.1929208040237427,-0.8178640007972717,-2.8256869316101074,0.26300981640815735,2.350184440612793,-2.3880128860473633,-2.657956600189209,1.5806409120559692,1.3346055746078491,-0.4425915777683258,-0.16022127866744995,2.709613561630249,0.989242672920227,-0.18375149369239807,0.6362038254737854,-2.4536545276641846,0.5379025340080261,-0.17263203859329224,0.20088624954223633,-1.8463140726089478,-0.07043513655662537,0.7850009799003601,-1.4243810176849365,1.6940208673477173,1.8265204429626465,1.0644088983535767,-1.478491187095642,-1.2350754737854004,3.30716609954834,-0.8892278671264648,1.489116907119751,-2.147594928741455,-0.2976296544075012,0.10676512867212296,0.2643774151802063,-0.6269505023956299,-1.3954969644546509,-2.2831332683563232,3.4362711906433105,-1.9476006031036377,-2.8959155082702637,-1.97703218460083,4.746956825256348,-0.62529057264328,-0.7299597263336182,-2.2860264778137207,0.8087806105613708,0.5496519207954407,-1.2942811250686646,-0.6376022696495056,1.0875178575515747,-0.5207781791687012,0.7687543034553528,-0.8318201303482056,1.283098578453064,2.168100357055664,1.0761439800262451,-0.1390324831008911,2.288144588470459,1.8665475845336914,-1.7534867525100708,-0.4850064814090729,-1.6711561679840088,-0.914482057094574,-0.031587935984134674,0.3210589289665222,1.2616490125656128,1.5903193950653076,0.35573241114616394,0.6769216656684875,0.5708574056625366,-1.215778112411499,-3.691640615463257,-2.8017666339874268,-0.7299203276634216,0.7529690861701965,-1.1922458410263062,0.13336507976055145,2.0104596614837646,0.864274263381958,0.0599142387509346,-3.242542028427124,0.14307211339473724,-1.1769477128982544,4.071067810058594,-0.9934900999069214,0.2989234924316406,0.24565449357032776,-2.660701036453247,-1.6034295558929443,-1.1491320133209229,2.4987616539001465,1.1848328113555908,1.544461965560913,-4.8104023933410645,-0.05565619096159935,-2.343412399291992,-0.07163424044847488,1.5719910860061646,1.389297366142273,0.1640150398015976,2.2448129653930664,-1.0913150310516357,-0.8404234051704407,-1.525995135307312,1.8476589918136597,0.5320829153060913,0.44004687666893005,0.9790820479393005,-0.9324285387992859,0.10567037761211395,0.9202806353569031,-1.6712384223937988,-0.09507101774215698,0.1474718302488327,0.5231088399887085,0.1718742698431015,0.7523815035820007,-0.5147140026092529,-2.0823721885681152,0.6418138146400452,1.3092677593231201,-2.855574607849121,2.354985475540161,-2.038146734237671,-2.226205825805664,2.909717321395874,0.33479371666908264,-1.2713382244110107,0.6674020290374756,-3.110997200012207,-2.339648723602295,-2.354351282119751,-3.154137372970581,2.1878907680511475,-1.0924429893493652,-4.0606207847595215,-2.574683904647827,0.3170689344406128,-0.06622566282749176,0.7361963391304016,0.26378801465034485,2.133416175842285,-0.4207685589790344,0.0731477290391922,-0.8496015667915344,-0.15328679978847504,1.328489065170288,1.7967166900634766,0.03379299119114876,-0.1843353807926178,1.5192232131958008,0.7771657109260559,-0.5859038233757019,-1.4134272336959839,-0.16503220796585083,-2.101086378097534,-1.0756748914718628,-1.0112557411193848,0.4132809638977051,0.23024675250053406,-1.0559592247009277,0.4213157892227173,0.7250505089759827,0.8363067507743835,0.4481644332408905,2.112819194793701,5.2101335525512695,1.7144379615783691,-0.8011800646781921,1.6371983289718628,-0.5338468551635742,-0.31565210223197937,0.022679496556520462,-0.7804953455924988,0.07133464515209198,0.3224675953388214,0.11544062197208405,0.15912577509880066,-0.24555929005146027,-0.6708812713623047,-1.1763734817504883,-1.6470749378204346,1.4332560300827026,-1.7862917184829712,0.8492525219917297,-2.283234119415283,-0.8025126457214355,3.3928182125091553,-1.155999779701233,-1.9608299732208252,-0.026584567502141,-0.17102371156215668,-2.6775097846984863,0.9344161152839661,0.930080771446228,-0.9393861293792725,1.1752065420150757,-1.9763023853302002,-1.22337806224823,-0.04496711865067482,-0.816360354423523,-0.364161878824234,1.6059370040893555,2.1356520652770996,2.4004273414611816,-2.98101806640625,0.40568071603775024,-2.920457601547241,-2.8174173831939697,0.5272707939147949,-0.35287022590637207,1.398055076599121,-2.778264045715332,0.8622322678565979,-0.10802368074655533,-1.146609902381897,-3.525280714035034,1.1940640211105347,-0.7086296081542969,-1.9705965518951416,-1.5217492580413818,0.7345380187034607,-0.429127037525177,1.0056129693984985,0.5310340523719788,-1.0283331871032715,1.0499796867370605,-0.5845990180969238,-2.802335739135742,0.3930145502090454,1.982957363128662,1.6826565265655518,0.4064638614654541,-1.7701963186264038,-0.20559799671173096,-0.3301616609096527,-1.8198697566986084,-0.3918043375015259,-1.4753092527389526,-0.08730272203683853,0.04332403466105461,0.36714980006217957,-1.8962093591690063,0.7139298319816589,-0.34090253710746765,-0.33329272270202637,0.5069292187690735,-1.5320085287094116,0.6644268035888672,-0.26073816418647766,-1.0951286554336548,-0.7941194176673889,-0.6402750611305237,1.044999361038208,3.418294668197632,-0.07058534771203995,0.6275712251663208,-1.1456596851348877,-0.4370262622833252,1.4595341682434082,1.4597699642181396,0.15530921518802643,-2.354408025741577,-0.1616477370262146,-2.846745491027832,2.3254928588867188,-2.971189260482788,-1.9494004249572754,0.8922866582870483,0.7361873388290405,-2.197381019592285,-1.1818758249282837,0.4804830849170685,1.393573522567749,-0.07373744249343872,-0.8207337856292725,0.5500155091285706,-0.1567470282316208,-1.6518666744232178,0.9623996615409851,-3.210926055908203,0.30223605036735535,1.3843011856079102,-1.388797402381897,1.3815268278121948,1.2889128923416138,-0.8017433881759644,1.210681676864624,1.988903284072876,-0.8279277682304382,-0.07400205731391907,0.3223193883895874,-0.27606987953186035,-3.8320581912994385,0.786963164806366,-0.44846346974372864,2.014631986618042,-0.061852507293224335,3.1660003662109375,-1.334477424621582,0.5522476434707642,2.105058431625366,-0.4444650709629059,-0.7061144709587097,0.44162142276763916,-2.162433624267578,-0.6816060543060303,-1.5780725479125977,-1.702023983001709,0.9934551119804382,-1.444183111190796,0.008710936643183231,0.8600379228591919,1.9922374486923218,0.2916768491268158,1.9118280410766602,-0.3329492509365082,-2.4008708000183105,-1.7213507890701294,0.7777366042137146,0.30957338213920593,0.36725306510925293,4.236534595489502,0.9175767302513123,1.085798978805542,-1.5326905250549316,-3.0571534633636475,3.0617740154266357,0.1542082577943802,0.916703462600708,-0.8636884093284607,-0.11188134551048279,1.7435567378997803,0.7180668711662292,-0.5474219918251038,1.0720369815826416,0.8784494400024414,0.28431573510169983,0.45535120368003845,1.9903992414474487,0.19280123710632324,-0.4338485598564148,3.4982213973999023,0.7818580269813538,-1.2622077465057373,-0.8677887320518494,-0.7708274722099304,1.604738712310791,-0.24318888783454895,1.1975231170654297,1.8842380046844482,2.1993143558502197,-1.5226387977600098,0.4747534394264221,-1.002948522567749,0.178488627076149,-1.7108336687088013,-0.32936739921569824,1.8871861696243286,-1.4224776029586792,0.13149988651275635,2.3239617347717285,-1.647881031036377,-1.8396214246749878,-1.3969234228134155,3.5668253898620605,-1.3638111352920532,-2.029742956161499,-1.0888046026229858,1.378379225730896,0.6756035089492798,0.4381179213523865,-0.1720139980316162,0.18667051196098328,1.7183514833450317,-1.4436861276626587,-2.283606767654419,-3.2309322357177734,0.616975724697113,-0.6287000775337219,-0.4336150884628296,1.292157769203186,2.7981534004211426,-0.529525637626648,1.1918386220932007,1.9470798969268799,0.5614616274833679,-1.25070059299469,-1.9584829807281494,0.7280385494232178,0.4376009404659271,0.1037508100271225,0.6677259206771851,1.9505438804626465,-0.9747112989425659,-2.046078681945801,-0.6399199366569519,1.0369993448257446,3.0902976989746094,-0.08211417496204376,1.7464624643325806,0.9153082966804504,-1.381803035736084,3.3091182708740234,-1.4106934070587158,4.258778095245361,1.3499072790145874,1.284412145614624,-1.2636781930923462,0.43326398730278015,2.5196900367736816,0.5690329074859619,0.4738799035549164,-2.8281972408294678,0.3126083314418793,0.9706709980964661,-1.4179742336273193,1.1196333169937134,-1.513770580291748,2.0786781311035156,1.6042351722717285,1.1686874628067017,-1.9348548650741577,-1.7747708559036255,1.4973164796829224,0.4545857310295105,1.645288109779358,0.8196117877960205,-0.8847696781158447,-1.3232520818710327,-0.16722893714904785,0.9955206513404846,0.04415509104728699,1.0873076915740967,-0.33078885078430176,-0.6285040974617004,0.42831215262413025,-0.2190283238887787,1.5485097169876099,0.6352736949920654,0.6249074935913086,-0.5486202836036682,-1.0839372873306274,1.3425770998001099,1.9526513814926147,-0.9444433450698853,1.5000001192092896,1.3323290348052979,-1.0390547513961792,-1.1952288150787354,-1.4034371376037598,-0.4226768910884857,1.1347726583480835,-0.7355678677558899,1.5870730876922607,1.1263636350631714,0.027247615158557892,2.0342345237731934,1.746040940284729,-1.4855341911315918,0.3138360381126404,-1.1384477615356445,-0.026251237839460373,-1.2121987342834473,-2.3549063205718994,-0.3570445775985718,1.7093299627304077,-0.4866752624511719,0.24979402124881744,0.087567538022995,2.5284128189086914,-0.29228097200393677,-1.5016553401947021,-1.856284260749817,-3.1739537715911865,-0.2946224510669708,1.319301962852478,-1.7132593393325806,-0.8400827646255493,-1.8251498937606812,-0.3187611997127533,0.6962437629699707,-0.8498163223266602,-3.0023298263549805,0.5634508728981018,0.3724282681941986,-0.5995075106620789,-1.5533140897750854,-2.131761312484741,-1.265820860862732,-0.9449432492256165,1.6702176332473755,0.18924599885940552,0.7170536518096924,-2.5550122261047363,-3.097778797149658,-0.020240943878889084,-0.1452285498380661,-1.4373877048492432,2.5365660190582275,0.5720508098602295,1.8910387754440308,-1.0453799962997437,-0.6568291187286377,-1.2836930751800537,1.4346193075180054,0.641732931137085,1.3169801235198975,-1.4141113758087158,2.9221127033233643,-0.4104962646961212,0.3095872402191162,-2.009050130844116,0.7103880643844604,-0.7194178700447083,-0.08301206678152084,-0.24942520260810852,3.104525566101074,-0.45767003297805786,-0.18372982740402222,-0.6537446975708008,-1.9774006605148315,-0.7115166187286377,-0.981920063495636,0.3030391037464142,-1.4411289691925049,-2.114062547683716,0.4719516336917877,1.33642578125,1.6087722778320312,4.126358985900879,2.289257287979126,1.277596354484558,-0.3174149990081787,-0.056658271700143814,1.9809398651123047,0.7620483040809631,1.752387523651123,0.8736608028411865,0.5690212845802307,0.9786925315856934,2.6578216552734375,3.1244263648986816,0.9304985404014587,-2.4312326908111572,-2.223081588745117,2.3994033336639404,-0.1539674997329712,1.2676599025726318,2.8306705951690674,0.05995986983180046,-0.16805607080459595,2.81548810005188,2.806710958480835,-3.1773955821990967,0.016855189576745033,3.087179183959961,1.136326551437378,0.884839653968811,1.9309905767440796,0.8717233538627625,2.2093427181243896,1.6459401845932007,0.36828726530075073,4.458303928375244,-2.3286290168762207,3.261965751647949,1.090916395187378,1.5225932598114014,-3.510695457458496,-0.6551579833030701,-3.6033260822296143,-1.2738909721374512,1.38838529586792,-1.070621371269226,-0.9334783554077148,-0.15975961089134216,3.0722360610961914,-0.23071947693824768,1.3297864198684692,-0.33468297123908997,2.7401938438415527,3.9209537506103516,0.5448909401893616,-0.809100329875946,3.4279534816741943,1.1606471538543701,0.2644434869289398,-0.8642982840538025,2.766619920730591,-1.2845032215118408,0.30453649163246155,0.026935404166579247,0.5977219343185425,0.09624332934617996,0.597713828086853,0.6598204970359802,2.663496971130371,3.0719616413116455,0.40827295184135437,1.0220352411270142,1.5127416849136353,-0.2026224583387375,-0.5201455950737,0.1729782223701477,-2.3248915672302246,-2.672081470489502,-1.8048754930496216,-0.8797140717506409,-0.269730806350708,2.065052032470703,2.6932032108306885,1.4146815538406372,0.7015942931175232,-2.717646360397339,0.40181541442871094,-0.31978464126586914,-1.6856507062911987,1.7247141599655151,1.7287774085998535,-1.7100247144699097,0.9326944947242737,1.340347170829773,0.774769127368927,-1.7038359642028809,-1.4676752090454102,-1.2580609321594238,-2.867236375808716,0.6432900428771973,-0.6996336579322815,-3.388505458831787,2.34077525138855,-1.3844969272613525,3.1151862144470215,3.4103546142578125,0.31476259231567383,0.018904026597738266,-1.4022862911224365,-0.4264910817146301,-0.15413840115070343,-0.6160900592803955,3.4670732021331787,-3.421567678451538,-2.3894147872924805,-0.42127734422683716,-0.6474563479423523,-2.923664093017578,-0.806831419467926,0.05323819816112518,0.20962999761104584,-0.7885605096817017,-0.7074299454689026,-0.7924751043319702,0.45294684171676636,1.5100481510162354,-0.962963879108429,-0.5081662535667419,0.41329649090766907,1.6444900035858154,-0.31691303849220276,1.1888365745544434,0.9388697147369385,0.07039181143045425,-1.1500475406646729,0.8553500771522522,-0.18600499629974365,-3.9549331665039062,-0.08265775442123413,2.645397424697876,0.15715304017066956,2.939474105834961,3.804142713546753,-0.7012847065925598,2.6238396167755127,0.43612849712371826,-1.1014777421951294,1.8268799781799316,0.8933549523353577,2.228327751159668,-1.0670604705810547,-1.5465867519378662,-0.6615039706230164,0.3811790943145752,-0.7763226628303528,0.8549550771713257,-2.2521536350250244,-0.2699078917503357,1.1208902597427368,-0.3291691243648529,-0.3679468035697937,1.155482292175293,-0.6850070357322693,0.4525246322154999,-0.5533595681190491,-0.730753481388092,-1.2821600437164307,-1.1466991901397705,5.429328441619873,0.9328965544700623,1.3136684894561768,0.9783328771591187,1.570053219795227,-2.55342173576355,1.0547676086425781,-0.4016762971878052,0.8879795074462891,2.7580435276031494,1.2365413904190063,-1.220597743988037,1.784444808959961,-0.6368674635887146,-0.6758208870887756,-0.636130154132843,-1.5214569568634033,-0.7053735256195068,-1.780892252922058,-0.7000772953033447,-2.011075019836426,-0.14038023352622986,0.7792121767997742,0.8763003349304199,1.7057018280029297,0.5881487727165222,-1.59486722946167,-1.5550440549850464,-0.11543314158916473,0.17348243296146393,-0.09280828386545181,1.0763013362884521,0.2103724628686905,-0.7822578549385071,-0.6508819460868835,1.4007354974746704,0.7308596968650818,2.38506817817688,0.6476519703865051,0.40006157755851746,-2.9895966053009033,0.47287505865097046,2.0397307872772217,0.5179478526115417,-0.715681254863739,1.5906851291656494,-1.6061879396438599,0.2308303266763687,0.30279576778411865,-3.044332981109619,0.08708380162715912,0.6461628079414368,-1.1315547227859497,0.5073145627975464,-1.8092458248138428,-1.0657228231430054,1.2164554595947266,-0.6169759035110474,-0.41258540749549866,1.1111904382705688,1.224632978439331,-1.3609410524368286,-1.9132416248321533,-2.05964732170105,-2.1160807609558105,1.1353282928466797,1.0953327417373657,-1.5964021682739258,-1.0461467504501343,2.055436134338379,1.6758273839950562,-1.859836459159851,-2.250559091567993,-0.5340801477432251,1.1997957229614258,-2.223496913909912,0.25216782093048096,0.8439813256263733,2.7118842601776123,1.1270198822021484,1.824187159538269,-0.6280524134635925,-0.19393566250801086,1.4456908702850342,2.385816812515259,1.124257206916809,-3.090635299682617,1.0578566789627075,-1.434048056602478,1.15287184715271,2.555241823196411,0.6559642553329468,-1.7495282888412476,1.105849266052246,-0.6567093133926392,-2.181048631668091,1.2777796983718872,0.260616272687912,-0.8309446573257446,1.2955543994903564,0.1460333615541458,-0.16513361036777496,0.08539334684610367,-0.3761845827102661,0.24489504098892212,4.0755438804626465,1.715093731880188,2.231088399887085,-1.1282156705856323,0.02249402180314064,0.5645909309387207,0.9690773487091064,0.14651261270046234,0.9433324337005615,0.7030926942825317,2.253187417984009,3.5226869583129883,-1.3181477785110474,-0.4102899134159088,-2.3329544067382812,0.9003168940544128,-0.1485118269920349,-0.3244231641292572,0.8163796067237854,-0.8059313297271729,-2.26249098777771,-0.7353485226631165,0.3795987367630005,0.6548874974250793,0.19823025166988373,-1.121172547340393,1.2581158876419067,-0.9255068898200989,0.5672868490219116,0.43710559606552124,-0.6632261872291565,-0.15703348815441132,0.5617786049842834,1.1183300018310547,-0.030429551377892494,0.0946401059627533,-1.1856662034988403,-0.8888365626335144,0.7429475784301758,1.8372365236282349,-0.4585375487804413,-1.51141357421875,-1.3100093603134155,-3.662015199661255,0.19180430471897125,-0.3315661549568176,0.9388668537139893,-1.711321473121643,-3.7525672912597656,2.4574692249298096,1.534929871559143,-0.515514075756073,1.2603440284729004,-0.9636101722717285,-1.3302597999572754,1.0438580513000488,0.3079584836959839,-1.6197528839111328,1.0370213985443115,-0.872948169708252,0.4934152066707611,0.8543235659599304,-0.22719848155975342,3.770726203918457,-1.489074945449829,3.1886134147644043,0.9326369762420654,-0.4538774788379669,-1.6607718467712402,0.5412864089012146,0.4930829405784607,0.7171199321746826,1.4064997434616089,-1.0723376274108887,1.42414128780365,-0.4475518465042114,0.09529305249452591,1.3366466760635376,1.458237648010254,-1.1383988857269287,3.3131601810455322,-0.3821967542171478,-1.9739967584609985,1.8619496822357178,0.07479923218488693,2.835899591445923,1.4044772386550903,0.36825627088546753,1.3711553812026978,-0.6781144738197327,-0.4228469133377075,1.6591858863830566,1.2534255981445312,-0.7305383086204529,-1.5466805696487427,-1.9755724668502808,2.397200584411621,-0.7803278565406799,-0.287392258644104,-1.230708122253418,-0.5844675302505493,2.0698413848876953,-0.08770833164453506,0.5343651175498962,-0.6050699353218079,-1.9074760675430298,1.091294527053833,0.06212989613413811,-0.3779374957084656,-0.060621920973062515,-1.0799835920333862,0.0833180770277977,0.7125962972640991,0.33925846219062805,-0.36326444149017334,2.3618695735931396,0.7748237252235413,3.2630434036254883,0.47519588470458984,0.42410987615585327,2.671434164047241,0.0813661441206932,-1.2570010423660278,-1.3083148002624512,-2.0459232330322266,0.7371697425842285,-0.20690034329891205,0.24924395978450775,0.3244527578353882,-4.641307830810547,0.36062660813331604,1.2170823812484741,0.1660405546426773,1.2346525192260742,-0.9597700238227844,2.0812478065490723,1.969218134880066,-1.471946120262146,3.9077537059783936,-0.5525514483451843,0.3166027069091797,0.41620171070098877,0.2196487933397293,1.2507076263427734,-0.0866437703371048,1.4438717365264893,-1.0602389574050903,-2.9951512813568115,1.453536868095398,1.2305752038955688,-1.0088133811950684,-0.9476311802864075,-2.549405336380005,-0.030528824776411057,-1.2596567869186401,-1.2152138948440552,-1.3637336492538452,-0.2737780511379242,-2.299679756164551,0.28132858872413635,-0.2678999602794647,0.3980849087238312,-0.9181956648826599,-2.067135810852051,0.5321284532546997,-2.3008177280426025,-2.3408312797546387,-2.048375368118286,0.445208340883255,0.6220364570617676,0.4712268114089966,1.1083658933639526,-1.6088076829910278,-1.2814186811447144,0.9587922096252441,-1.1628609895706177,-1.0720815658569336,-0.5634162425994873,-1.690563678741455,-1.0364669561386108,1.1041277647018433,-0.4228397607803345,0.0540582612156868,-1.1692551374435425,-1.5298527479171753,-1.6204438209533691,1.9147589206695557,-3.538752794265747,0.5164157152175903,0.4691425561904907,1.1089730262756348,1.2875874042510986,-1.9580543041229248,-0.37594759464263916,-0.5346958041191101,-1.2320411205291748,0.7067042589187622,1.0218924283981323,0.7624021768569946,-1.1891220808029175,-0.22892700135707855,-2.467775821685791,1.2769821882247925,1.6414234638214111,0.7412848472595215,0.7837104201316833,-0.3167799711227417,-1.2035424709320068,0.9913356900215149,-1.069964051246643,-2.8718371391296387,-0.1915176659822464,-0.019208993762731552,-1.0346394777297974,-0.10826581716537476,0.3031642735004425,0.3708696961402893,-0.007776114623993635,-0.8257055282592773,0.2575238049030304,1.4690990447998047,0.93545001745224,-0.9734775424003601,1.4039230346679688,0.444443017244339,2.546437978744507,0.7105774283409119,0.7147196531295776,-0.18949013948440552,1.4022327661514282,-0.5182533264160156,-0.7576921582221985,-1.3552418947219849,0.006715054623782635,0.5608171224594116,0.03375497832894325,-1.7925255298614502,-0.5056480169296265,1.7416421175003052,0.7545045018196106,0.02394920587539673,1.9422967433929443,-1.2313677072525024,0.23280315101146698,-1.2686916589736938,-1.749295711517334,0.019700871780514717,-2.2499427795410156,-0.07753502577543259,-0.08026216924190521,1.2373470067977905,-2.1991586685180664,-0.9239299297332764,-0.8516882061958313,-0.03863101080060005,0.44978174567222595,0.1537521481513977,1.251002311706543,-0.4786774814128876,-1.1548773050308228,-0.11181439459323883,-1.3209888935089111,0.2573622465133667,0.5312804579734802,-0.2158535271883011,-0.03757786378264427,2.4083869457244873,-0.5941406488418579,1.6092896461486816,-1.6317287683486938,-0.41875985264778137,-0.5114682912826538,-1.077402114868164,-1.3027361631393433,1.8470131158828735,-1.923754096031189,-0.05786065012216568,-2.2264938354492188,-2.3625941276550293,-2.1496520042419434,1.840383529663086,-0.8170472383499146,-0.9971511363983154,0.35871678590774536,-2.51682186126709,1.8701227903366089,0.42430031299591064,0.44949647784233093,-0.4234733581542969,-0.1335594654083252,2.0446131229400635,-0.4790068566799164,0.9122400283813477,-1.6386438608169556,-1.1705209016799927,1.044480800628662,-1.5093365907669067,-0.17707663774490356,-1.361350178718567,-0.5700942277908325,1.9500670433044434,-2.000582456588745,-0.633296012878418,-1.5079947710037231,1.7615238428115845,-1.4813194274902344,-2.1128103733062744,2.9325430393218994,0.6676537990570068,-0.8343838453292847,1.189988374710083,1.4355300664901733,3.4601340293884277,-0.7567875981330872,-0.100308857858181,1.325978398323059,0.4265439808368683,-0.8596279621124268,-2.1184160709381104,-2.247054100036621,-1.1353330612182617,0.09104534238576889,-0.6287233829498291,1.6857093572616577,1.8605722188949585,0.35786762833595276,-1.0287575721740723,-1.3831826448440552,0.48709917068481445,0.8629971146583557,0.4172830581665039,-0.27839046716690063,-0.2064671516418457,-0.1727200448513031,-0.3396832048892975,0.8349718451499939,-0.9235818386077881,0.9298434257507324,1.239424705505371,-0.5409731268882751,0.8241669535636902,0.38231202960014343,1.3079034090042114,-1.2257065773010254,-0.9686204195022583,-2.696546792984009,-1.0292302370071411,2.0699329376220703,0.34535762667655945,-0.42226657271385193,-1.4509413242340088,-1.060634970664978,-1.4710400104522705,-1.3522831201553345,0.5629730820655823,-0.2783972918987274,2.783860683441162,-0.7590618133544922,-0.7547767758369446,1.7191922664642334,-0.042332541197538376,1.5001178979873657,0.5634992718696594,2.9723060131073,-1.490028977394104,-0.4587669372558594,-1.323419213294983,0.684848427772522,1.7248923778533936,2.188537836074829,-0.27237391471862793,-1.382378339767456,2.1610031127929688,0.37298697233200073,1.0303257703781128,0.7696311473846436,-0.3341483473777771,-2.3238441944122314,-1.970458745956421,1.3704911470413208,0.027456002309918404,-0.19043363630771637,0.7504169344902039,-1.0580077171325684,0.5766159892082214,1.9684710502624512,1.5468273162841797,1.7704490423202515,1.76190185546875,0.6240589022636414,-1.0005309581756592,1.8462250232696533,-2.0502965450286865,-2.3134536743164062,-0.9415381550788879,-1.037145733833313,0.576000988483429,1.9641213417053223,0.8359832167625427,-1.1344226598739624,-1.147124171257019,-0.8621670603752136,2.5657691955566406,0.526699960231781,0.45415112376213074,-0.08508872985839844,0.8965414762496948,3.0680978298187256,-1.32084059715271,1.399416446685791,0.5234319567680359,1.697568416595459,-0.4067213237285614,-1.01364004611969,-0.1865508109331131,1.4623733758926392,1.8011969327926636,2.0847694873809814,0.15077707171440125,-1.52531099319458,-2.7573578357696533,0.5317202806472778,-0.1420687437057495,-1.717385172843933,1.2890450954437256,0.0016035225708037615,2.0491578578948975,2.946571111679077,0.8741344213485718,1.3502960205078125,1.2047803401947021,0.8299288153648376,2.3434195518493652,0.12062899023294449,1.0789051055908203,-1.4591280221939087,-1.301720380783081,0.76631760597229,-2.5024302005767822,0.3930920958518982,-0.6337535381317139,0.026976320892572403,-0.3757323622703552,2.164450168609619,-0.4497905373573303,-0.2385576218366623,-0.3062407076358795,-3.110609769821167,0.6104223728179932,0.43810126185417175,-0.15417394042015076,1.5309771299362183,-2.074155807495117,1.6561942100524902,1.2999552488327026,0.6790747046470642,0.5005945563316345,2.2140300273895264,-0.24803151190280914,-0.5020940899848938,0.9540426135063171,-1.5167410373687744,-1.298565149307251,-0.3365665674209595,2.3540122509002686,-1.811640977859497,1.3274818658828735,0.3844980299472809,-0.9974191188812256,-1.5371018648147583,1.22848641872406,0.44318681955337524,-0.11353999376296997,0.30156639218330383,-1.2896206378936768,0.19027327001094818,-1.1995893716812134,-1.2619359493255615,1.871605634689331,1.3050886392593384,0.7515777349472046,0.9569828510284424,-0.823397696018219,0.5366529822349548,2.8183157444000244,-0.3807689845561981,3.175489902496338,0.9185709953308105,-0.4946279525756836,-0.6778814792633057,-0.8049260377883911,1.0269391536712646,-2.3119277954101562,0.6216914653778076,3.5852956771850586,0.5476529598236084,0.7699590921401978,-0.11584626138210297,-0.6532273888587952,-0.14851003885269165,1.2081047296524048,-2.343487501144409,-2.241433620452881,0.027503229677677155,1.2465132474899292,0.7126920223236084,1.412318468093872,0.8133314847946167,0.33489659428596497,0.0495927557349205,1.2855433225631714,1.6015864610671997,-0.7966509461402893,1.660284161567688,-0.16018593311309814,3.4813551902770996,-0.37991535663604736,-0.7592726349830627,-0.13771195709705353,-0.7367451190948486,-0.6661337018013,0.3372074067592621,-0.09233197569847107,0.7148439884185791,0.575299859046936,0.6584559679031372,-1.570062279701233,0.39457789063453674,0.3757915496826172,-1.8582375049591064,1.6804927587509155,0.16207261383533478,-0.7712585926055908,-0.8250336050987244,1.5406372547149658,0.7003549337387085,1.9280837774276733,2.506049871444702,2.1762301921844482,0.20011217892169952,2.1168622970581055,0.02794683538377285,0.7160906791687012,1.5340691804885864,-0.36944106221199036,1.1613569259643555,-0.8270218968391418,-1.2104518413543701,-0.41146862506866455,0.7615829706192017,-0.10162810236215591,1.7802538871765137,0.298628032207489,0.417243093252182,-2.5583486557006836,-1.0174200534820557,-0.5827004313468933,-0.981884241104126,-0.17744950950145721,-1.3832676410675049,-2.3940978050231934,1.380103588104248,-0.6671773791313171,0.03921310231089592,-1.1174230575561523,0.2613932192325592,0.8253339529037476,0.011469954624772072,-0.14739926159381866,0.1733018159866333,0.06003008410334587,0.3465266823768616,2.005457639694214,0.40077126026153564,1.4304308891296387,3.096632480621338,-1.3879145383834839,-0.8407222032546997,0.3859945237636566,0.607629120349884,1.9856929779052734,2.238565444946289,-1.2157177925109863,0.7871907353401184,-0.21091876924037933,1.8656967878341675,2.5394411087036133,2.406388759613037,-0.0887976586818695,1.0268268585205078,-0.15082871913909912,-0.389774352312088,-1.2335671186447144,0.8077698349952698,-0.06890352070331573,0.42973440885543823,-1.0892997980117798,1.6628272533416748,0.6282690763473511,-0.03616280481219292,-0.3694082200527191,1.3478426933288574,0.006640185136348009,-0.19409824907779694,1.220760703086853,0.3853330910205841,1.93114173412323,0.4759119749069214,-0.2200189083814621,-0.5009161829948425,-0.6800342798233032,-1.4698694944381714,-1.1672965288162231,-1.7242600917816162,0.13660232722759247,0.11026735603809357,1.231744647026062,0.18163704872131348,-0.6508976221084595,-0.13399560749530792,-0.992179811000824,0.04863006994128227,0.717083752155304,4.8013129234313965,1.0224798917770386,-1.1586744785308838,-1.3200465440750122,1.523267388343811,0.5961615443229675,1.6447645425796509,1.8360514640808105,-1.4044996500015259,-1.5054409503936768,-0.028546910732984543,-0.6549668908119202,1.335800051689148,1.595207929611206,-1.7591800689697266,0.15297748148441315,2.9204208850860596,1.733771562576294,1.0564165115356445,-0.7340573668479919,-1.5811412334442139,2.222607374191284,0.7959738969802856,-5.160608768463135,-0.7750375866889954,2.1262574195861816,-1.7082937955856323,0.9470908045768738,0.5756990313529968,1.4676133394241333,0.5199952125549316,-0.025182688608765602,-1.2382392883300781,-0.893633246421814,0.23293724656105042,-2.393653154373169,-1.6482499837875366,-1.0156875848770142,0.4991454482078552,0.5560098886489868,2.1961588859558105,1.413684606552124,0.3880506157875061,0.27857574820518494,-1.5224684476852417,-1.3232792615890503,2.3561952114105225,2.956331491470337,0.09734875708818436,-0.42421895265579224,1.3204154968261719,-0.3426703214645386,-0.04127401486039162,0.2061374932527542,1.3847107887268066,-0.3774060308933258,0.43556809425354004,0.9028580784797668,0.15948805212974548,-0.060638125985860825,0.14210855960845947,-1.5504035949707031,0.8948978781700134,0.5688700079917908,-0.16528689861297607,0.5035598278045654,1.7179816961288452,2.4077091217041016,-0.2690857946872711,0.4741702377796173,1.6325265169143677,-1.6774041652679443,-0.9380784630775452,-1.3710050582885742,-2.790803909301758,0.4361262023448944,0.832798421382904,2.8849451541900635,1.2555553913116455,0.7829281091690063,0.21748720109462738,-0.5990194082260132,0.13959750533103943,-0.16727279126644135,0.7324151992797852,0.8810320496559143,1.557444453239441,0.3414972126483917,0.3465179204940796,-0.043480515480041504,-0.6902259588241577,-1.3298031091690063,-1.3019930124282837,2.900291919708252,1.7459146976470947,0.2815951108932495,-0.12345581501722336,2.1394965648651123,0.6090901494026184,-1.0166070461273193,-0.5847684144973755,1.3063123226165771,0.40825411677360535,-1.3844449520111084,0.35880225896835327,1.136318564414978,0.5197036266326904,-0.7536562085151672,0.19646163284778595],"xaxis":"x","y":[-0.007171851582825184,-0.01829364150762558,-0.08970219641923904,0.027330651879310608,-0.13484908640384674,0.02293999120593071,-0.03240957856178284,0.04698169603943825,-0.027838245034217834,0.03834183141589165,-0.05192026495933533,-0.056878529489040375,0.004290259908884764,-0.12311719357967377,-0.112371526658535,-0.025738606229424477,0.09423583745956421,-0.045479774475097656,-0.06203579157590866,0.014923668466508389,-0.01169584784656763,-0.13789136707782745,-0.02680972032248974,0.07064265012741089,0.010650824755430222,-0.12727536261081696,-0.06768280267715454,-0.01883491687476635,0.008913024328649044,-0.025050388649106026,-0.028949076309800148,0.012391860596835613,0.05615317076444626,0.041244424879550934,0.08354008197784424,-0.05718844756484032,0.00023891199089121073,-0.021038081496953964,0.0006428352789953351,0.010827010497450829,0.026242895051836967,-0.013066243380308151,0.03483196347951889,-0.11587335914373398,0.005438556428998709,0.0014752198476344347,0.0030189899262040854,-0.023286323994398117,-0.08072709292173386,-0.0577176995575428,-0.007073667831718922,0.03460077941417694,-0.010597634129226208,-0.11927873641252518,0.09357333928346634,-0.013680520467460155,-0.18997851014137268,-0.1110537126660347,0.07318563759326935,-0.049093008041381836,0.0031379354186356068,-0.10144437104463577,-0.014490229077637196,0.05326293408870697,0.029425831511616707,-0.05811077356338501,0.004843545146286488,0.04157397150993347,0.07021244615316391,-0.021838704124093056,0.059050749987363815,-0.020699452608823776,0.0075865196995437145,-0.05578646436333656,-0.01996009424328804,0.03370258957147598,-0.012240375392138958,0.05007241293787956,-0.05550346150994301,-0.027183400467038155,-0.015707317739725113,-0.054360855370759964,0.0341772623360157,-0.08437182009220123,-0.010597107000648975,-0.03176853433251381,-0.12511038780212402,-0.04785061255097389,-0.06676574796438217,0.0630040094256401,0.0264336746186018,0.053380582481622696,0.0036048342008143663,-0.047801900655031204,0.003576466115191579,0.0695524662733078,0.08827053010463715,0.05820884928107262,0.029063301160931587,-0.067078597843647,-0.014417633414268494,-0.016250640153884888,-0.06511057168245316,-0.09329292923212051,0.07435775548219681,0.05043691769242287,0.018677635118365288,0.047804318368434906,0.04335160180926323,-0.07436495274305344,0.06686381995677948,0.017950253561139107,-0.12278637290000916,0.08845892548561096,-0.07338162511587143,-0.04384586960077286,0.012357586063444614,0.02871166169643402,0.05419701710343361,0.11886250972747803,-0.02442646026611328,0.05332907289266586,-0.0862140879034996,0.042111821472644806,-0.03586370125412941,-0.058117855340242386,-0.025255070999264717,0.10236778110265732,-0.025989461690187454,-0.09070616215467453,0.01911025680601597,0.06473693996667862,0.04697452113032341,0.12113229185342789,-0.02189200557768345,0.0018683046801015735,0.10118546336889267,-0.04210778698325157,-0.14940907061100006,0.08482686430215836,0.08721166104078293,-0.10961536318063736,-0.1507931500673294,-0.0776013508439064,-0.13153226673603058,0.018710484728217125,-0.06682432442903519,0.03537610545754433,-0.09474178403615952,-0.06934944540262222,0.07816453278064728,-0.07804103195667267,-0.0321074016392231,0.027951044961810112,0.2001352608203888,-0.04815589264035225,-0.05436571687459946,-0.09411291778087616,0.08285703510046005,-0.05194491520524025,-0.026811620220541954,-0.15319380164146423,-0.04215623438358307,0.0550510510802269,0.03442595899105072,-0.059252191334962845,-0.10387323051691055,0.0005352628650143743,0.031752243638038635,0.0021746058482676744,0.04139390587806702,0.04006727784872055,-0.11561257392168045,-0.02114708535373211,-0.0256419125944376,0.09621739387512207,-0.01877553015947342,0.03132198750972748,-0.08619244396686554,-0.0071138362400233746,0.029175179079174995,-0.002996448427438736,0.04633649066090584,0.09097567945718765,-0.1320832371711731,-0.08515666425228119,0.06309602409601212,0.010651489719748497,-0.04360052943229675,-0.10084854811429977,-0.000700437172781676,0.023402418941259384,-0.031231805682182312,0.011199064552783966,-0.06540636718273163,0.05986581742763519,0.06331314146518707,0.03579145669937134,-0.01413256861269474,0.0685272142291069,-0.1533033698797226,-0.023508397862315178,0.10876509547233582,0.01582888513803482,0.0333494134247303,0.006426155101507902,0.13022689521312714,-0.018001137301325798,0.3073931634426117,0.03664019703865051,-0.02444884181022644,0.09544134885072708,-0.016416320577263832,0.0987069383263588,-0.013737057335674763,-0.01883813925087452,-0.0010426528751850128,-0.07060007005929947,-0.01409117504954338,-0.06180081143975258,-0.07308503985404968,-0.10332663357257843,-0.09690747410058975,-0.05435292795300484,0.09960050880908966,-0.0811159759759903,0.023255519568920135,-0.05193702131509781,0.09312302619218826,-0.024524467065930367,-0.14911063015460968,0.021855603903532028,0.04710846021771431,0.026871396228671074,-0.047187454998493195,0.08124981820583344,0.012727877125144005,0.04444846883416176,-0.02276092767715454,-0.07494175434112549,-0.025907984003424644,0.06361076980829239,-0.06249972805380821,0.012816629372537136,0.005452719517052174,0.036625936627388,0.11051809042692184,0.1378006935119629,0.09098298847675323,-0.07408244162797928,0.06491013616323471,0.05842381343245506,-0.06587867438793182,0.1317049115896225,0.08976396173238754,0.05467594042420387,0.09489697217941284,0.05878373980522156,-0.009978827089071274,-0.04113747179508209,0.07011668384075165,0.06796910613775253,0.30012476444244385,-0.11106665432453156,-0.010901600122451782,-0.017228830605745316,-0.06595997512340546,-0.062245920300483704,0.11908749490976334,-0.13562847673892975,0.07038360834121704,0.0553615428507328,0.05289289727807045,0.06281158328056335,0.07445458322763443,-0.011684495024383068,0.03412860259413719,-0.1593080312013626,0.004408707842230797,0.07082575559616089,-0.05260744318366051,-0.054190050810575485,0.12226852029561996,-0.16959473490715027,-0.043605878949165344,-0.041226502507925034,-0.038471803069114685,0.028988471254706383,-0.09795349836349487,0.03546610102057457,0.014103035442531109,-0.046469103544950485,0.00575448852032423,-0.008046135306358337,0.05154343694448471,-0.03358428552746773,-0.06976266950368881,0.05472542718052864,0.020934896543622017,0.032753054052591324,-0.011195572093129158,0.09182675182819366,-0.0305403470993042,-0.10324294120073318,0.0539865642786026,0.04522768408060074,0.046060945838689804,-0.06658650934696198,-0.0031636622734367847,-0.032327137887477875,0.08208506554365158,-0.10950886458158493,0.012828937731683254,-0.047409478574991226,-0.05609573796391487,-0.0024065657053142786,-0.056207865476608276,0.037895411252975464,0.08352316915988922,0.02394583262503147,-0.014276469126343727,-0.08750391751527786,-0.038265906274318695,-0.14350968599319458,-0.057017821818590164,0.034824639558792114,0.05529865249991417,-0.020408323034644127,0.024367840960621834,-0.016529183834791183,0.054881274700164795,-0.028886543586850166,0.15833251178264618,-0.01712632179260254,-0.10572176426649094,0.028696434572339058,0.0064409272745251656,-0.03923261538147926,-0.08155696839094162,0.06756320595741272,-0.09040090441703796,0.010750476270914078,0.1701875925064087,-0.005104001145809889,0.09731804579496384,0.03252941370010376,0.10636520385742188,0.030718360096216202,-0.086151123046875,0.12364938855171204,0.04554150626063347,-0.08942242711782455,0.04052296653389931,0.001376051572151482,-0.03298472985625267,0.1626570075750351,0.081228107213974,0.033075783401727676,-0.0020386269316077232,-0.06137900426983833,0.01228400506079197,-0.0850263386964798,-0.0536203570663929,0.011941241100430489,0.07043715566396713,0.14444656670093536,-0.10037000477313995,-0.05415615066885948,-0.10074848681688309,-0.18336854875087738,0.13145817816257477,0.01747782714664936,0.028182856738567352,0.19297060370445251,0.09158314019441605,0.08173531293869019,0.04881211742758751,0.007222830317914486,0.06837419420480728,-0.051415592432022095,0.03266541659832001,-0.07865811139345169,0.14287018775939941,-0.1018141582608223,0.04168819263577461,-0.0018047582125291228,0.11452756822109222,0.05844239145517349,-0.11514468491077423,0.04223380610346794,-0.09414311498403549,0.0763915628194809,0.05025225132703781,-0.04352349042892456,0.017145778983831406,-0.04341023787856102,-0.01923173852264881,-0.044027045369148254,0.03810412436723709,0.08775205910205841,-0.03100946545600891,0.04578924924135208,-0.035390276461839676,-0.04354533925652504,0.0822288766503334,0.028142226859927177,-0.03848782181739807,-0.028370672836899757,0.0013752869563177228,0.025128839537501335,0.031644660979509354,-0.02744789607822895,-0.032400161027908325,-0.005256555508822203,-0.02222464233636856,-0.002560701686888933,0.04957783967256546,-0.025095898658037186,-0.0057098460383713245,0.09130437672138214,0.05386578291654587,0.006347497925162315,-0.03919896483421326,-0.04096797853708267,0.004241400398313999,-0.03407667949795723,0.08810824155807495,0.011828589253127575,-0.07512425631284714,-0.09702446311712265,-0.00389279006049037,-0.03268122673034668,-0.0695895329117775,0.03036162443459034,-0.03597339615225792,0.022672109305858612,0.022132303565740585,-0.02102242037653923,-0.14095871150493622,0.01619761250913143,0.11768566817045212,0.04969373717904091,0.016024582087993622,0.0837516263127327,0.15325768291950226,-0.11570132523775101,-0.1016731783747673,-0.11662662029266357,-0.09306547045707703,-0.03695470839738846,0.010138370096683502,0.0822567269206047,-0.059637486934661865,0.10038521140813828,0.061514630913734436,-0.02637978456914425,0.023916777223348618,-0.039456699043512344,-0.11040594428777695,-0.08112341165542603,-0.03216424584388733,0.05443523824214935,-0.10841377079486847,-0.012226132676005363,0.08294159919023514,-0.12132149189710617,-0.007118762470781803,0.12805411219596863,-0.0826200470328331,0.008408388122916222,-0.025961758568882942,-0.017920944839715958,-0.13417115807533264,-0.03615252301096916,-0.026398010551929474,-0.09926790744066238,0.10764101892709732,0.1082940325140953,0.03140699490904808,0.053797461092472076,-0.1268514096736908,0.11116408556699753,-0.08853773772716522,-0.02886720560491085,0.03403909504413605,0.058570973575115204,0.041326675564050674,0.11623529344797134,0.030230697244405746,0.013264346867799759,0.042333658784627914,0.049669038504362106,-0.023058680817484856,-0.03136126697063446,0.0035312960390001535,0.05902017280459404,-0.17204298079013824,-0.008347849361598492,-0.07770080864429474,0.02514544688165188,0.033495500683784485,-0.07590080052614212,0.002529878867790103,0.07507101446390152,-0.07122761756181717,0.05220809951424599,-0.13360437750816345,-0.0036384135019034147,-0.02662508375942707,0.22941848635673523,-0.06733808666467667,0.09503539651632309,0.012211790308356285,0.058259110897779465,0.021949375048279762,0.11136576533317566,-0.09508916735649109,0.06795931607484818,-0.03958987817168236,0.046481601893901825,0.022786784917116165,-0.09736756980419159,-0.1074548289179802,-0.041221924126148224,-0.0928904265165329,-0.08480209857225418,0.08990011364221573,0.01759841851890087,-0.10848995298147202,-0.04058288410305977,0.0314810536801815,-0.07034974545240402,0.04450702667236328,-0.042675089091062546,0.14750193059444427,0.06438159197568893,-0.014328517951071262,-0.07809288054704666,0.018149467185139656,0.08202740550041199,0.0027348152361810207,-0.010515976697206497,-0.1068699061870575,0.026028482243418694,-0.03615759313106537,-0.06147283315658569,0.1369733363389969,0.07273826003074646,0.09650608897209167,0.05994836986064911,0.019246207550168037,0.03227188438177109,-0.04472336918115616,0.00031087687239050865,-0.02694663591682911,0.0658368170261383,-0.07397035509347916,0.11664678901433945,-0.024608787149190903,0.04752553626894951,-0.09828563034534454,-0.0370369479060173,-0.011008011177182198,-0.16117945313453674,-0.05963890254497528,-0.007504311390221119,-0.07819858938455582,-0.029003102332353592,0.06742880493402481,0.018638504669070244,0.01064610481262207,0.028439130634069443,-0.09996653348207474,-0.0312977135181427,0.0034964929800480604,0.035320013761520386,-0.16805621981620789,-0.15987081825733185,-0.01864507794380188,-0.008377843536436558,0.10135652124881744,0.03394285589456558,-0.018824828788638115,-0.028282849118113518,0.09232091903686523,-0.011887419037520885,-0.04301294684410095,0.10435076802968979,-0.06834176927804947,0.09181039780378342,0.09606637805700302,0.11046610027551651,0.05951075255870819,0.1113971695303917,-0.001867004088126123,0.03050198219716549,-0.025282396003603935,0.03879081457853317,-0.05062631517648697,-0.0014234347036108375,0.03837181255221367,0.008533918298780918,0.016150668263435364,0.053817979991436005,0.023864636197686195,-0.05227596312761307,-0.081818126142025,-0.10745721310377121,-0.08764946460723877,-0.027260329574346542,-0.005450253840535879,-0.08160126209259033,-0.04935859143733978,-0.03048117831349373,0.09960892796516418,-0.021474050357937813,-0.0891445130109787,-0.06114207208156586,0.10777442157268524,0.0661686584353447,-0.0979076474905014,-0.06114271283149719,0.13736416399478912,0.05193566158413887,-0.029873361811041832,-0.0800601914525032,0.11757543683052063,0.05805766582489014,-0.04332973062992096,-0.06009475141763687,0.03770051151514053,0.022390680387616158,0.04761193320155144,0.0731896311044693,0.06226352974772453,-0.06507395952939987,0.014061147347092628,-0.09054748713970184,0.050409264862537384,0.11043423414230347,-0.07168833166360855,0.019292810931801796,-0.04112383723258972,-0.016115274280309677,0.009978998452425003,-0.010861270129680634,-0.04139456897974014,-0.0015180034097284079,0.07999828457832336,0.04094494879245758,0.12514998018741608,-0.15444917976856232,0.0051024542190134525,0.005322862882167101,0.06051988527178764,-0.03636778146028519,0.024188868701457977,0.04504988342523575,-0.046527277678251266,0.09268264472484589,0.05112412944436073,-0.09831780940294266,-0.017896905541419983,0.04241214692592621,-0.01993946172297001,0.005556163378059864,-0.14815686643123627,-0.011935890652239323,-0.08730320632457733,0.04424823820590973,0.013952056877315044,0.011362883262336254,0.06186437979340553,0.044371820986270905,0.0007361029856838286,-0.09360371530056,0.00017135714006144553,0.08966889977455139,-0.03580253943800926,0.0789531022310257,-0.0467827133834362,-0.14702728390693665,0.0641309916973114,0.003268091706559062,0.07348170131444931,-0.03984687477350235,0.00751391239464283,0.0033116538543254137,-0.0351669043302536,-0.002482770010828972,0.11396283656358719,0.018414901569485664,-0.17323783040046692,-0.1039784848690033,0.06947546452283859,-0.002588060684502125,-0.05328470841050148,-0.024816585704684258,0.07776976376771927,-0.013433554209768772,0.00690848333761096,0.10543720424175262,0.0024729894939810038,0.02677414007484913,-0.14230026304721832,-0.0721016526222229,0.012778031639754772,0.010541168972849846,0.05599045380949974,0.019578317180275917,0.06276632100343704,-0.1782628744840622,-0.14962579309940338,0.05443934351205826,0.07263199985027313,0.06158048287034035,-0.1919507384300232,0.00956734362989664,0.06207088753581047,-0.0641898512840271,-0.015160479582846165,-0.003468221751973033,-0.01433200016617775,-0.12259302288293839,0.14832672476768494,-0.1645837277173996,0.05486200377345085,0.016000617295503616,-0.003432386089116335,0.0998527854681015,-0.052552834153175354,-0.009063396602869034,-0.09124714881181717,-0.07038339972496033,-0.045927923172712326,0.023745672777295113,-0.10340216010808945,0.06338600069284439,0.09072265774011612,0.019769679754972458,0.0703735426068306,-0.008026571944355965,-0.14341400563716888,-0.04430647939443588,0.052395593374967575,0.07770319283008575,0.036735620349645615,0.00725205522030592,-0.05450191721320152,-0.003496431978419423,-0.09914780408143997,0.030305424705147743,0.10891042649745941,0.07134488224983215,-0.06496427953243256,0.005776699166744947,0.04250630363821983,0.041752126067876816,0.062031108886003494,-0.12036773562431335,-0.0035878042690455914,0.02759537659585476,-0.10251359641551971,-0.1206788420677185,0.09558899700641632,0.1560204029083252,0.05903654173016548,-0.09803599864244461,0.13188816606998444,0.12425636500120163,-0.05104450136423111,-0.06326637417078018,0.040274083614349365,0.11770038306713104,0.07835029810667038,0.0050908103585243225,-0.06708163022994995,-0.07063473761081696,0.04718463495373726,-0.17607982456684113,0.06580587476491928,-0.008560971356928349,-0.207789808511734,-0.00556411687284708,0.03790935501456261,0.11476418375968933,0.0221320278942585,0.07442063093185425,0.1047285720705986,-0.03709602728486061,-0.0775509849190712,-0.007109971717000008,0.025766009464859962,-0.08114557713270187,-0.011185417883098125,-0.08445093035697937,0.10586948692798615,-0.09064866602420807,-0.010192646645009518,0.06769900023937225,0.0426904521882534,0.09156596660614014,-0.04096714407205582,0.04340473562479019,0.05134022608399391,-0.02362024411559105,-0.17397929728031158,0.09978288412094116,-0.026144394651055336,0.10566964000463486,-0.023834826424717903,0.1335563212633133,-0.07297571748495102,0.037307292222976685,0.020142139866948128,-0.03779204189777374,0.09273377805948257,0.0482143759727478,-0.014578751288354397,0.06576327979564667,-0.036488186568021774,-0.14139381051063538,-0.09172928333282471,-0.060295604169368744,-0.013891387730836868,-0.17899024486541748,0.03799828886985779,0.05961943417787552,0.04560321941971779,0.05025070533156395,-0.11860565096139908,0.0673588365316391,0.11187024414539337,0.02610080875456333,-0.0533904992043972,0.10540230572223663,0.005289286375045776,0.10350580513477325,0.18476495146751404,-0.12223018705844879,-0.13451288640499115,0.0024227178655564785,-0.006592646706849337,-0.016357116401195526,-0.014445648528635502,0.02647440880537033,-0.031072115525603294,-0.13344432413578033,-0.12509620189666748,-0.023000631481409073,0.04927903041243553,-0.04012301564216614,-0.07083030045032501,-0.13227765262126923,0.0630912035703659,0.053238071501255035,-0.020587945356965065,-0.02781209908425808,-0.08827578276395798,-0.10589415580034256,0.012016760185360909,0.0008014632621780038,-0.09972968697547913,0.033367276191711426,0.06316876411437988,0.09551473706960678,-0.07605060935020447,0.1199551671743393,0.030777839943766594,0.026443270966410637,0.027439117431640625,-0.057788509875535965,-0.025696663185954094,0.16543157398700714,-0.0302549097687006,-0.058504536747932434,0.026167606934905052,0.036337852478027344,-0.0859520211815834,0.017129426822066307,-0.003163970308378339,0.05977395176887512,0.04464893043041229,-0.018693551421165466,0.035594891756772995,-0.13224436342716217,0.08988095819950104,-0.1211571991443634,0.04998158663511276,-0.1360376924276352,0.023620480671525,0.03753281384706497,-0.054831087589263916,-0.02014198526740074,-0.0915258601307869,0.10978009551763535,0.08924674242734909,-0.058717891573905945,-0.04203663393855095,0.06216374412178993,-0.0004670726484619081,0.04834358021616936,0.0013172870967537165,0.13238264620304108,-0.0005043585551902652,-0.006954247131943703,0.013262007385492325,-0.14111533761024475,0.00851445458829403,0.008245740085840225,-0.11748337745666504,-0.005989997182041407,-0.07536248862743378,-0.0781550481915474,0.012676927261054516,-0.08478797227144241,-0.06594464182853699,0.11085259169340134,-0.01700627990067005,-0.0243147574365139,-0.11317168921232224,0.114311084151268,0.0011490003671497107,-0.08004923909902573,0.13405200839042664,0.09679491817951202,-0.05599131062626839,0.026629885658621788,0.19191643595695496,-0.1002182587981224,-0.0014919934328645468,0.01570621319115162,0.13579314947128296,0.10184634476900101,-0.040765855461359024,0.03289655223488808,-0.049611896276474,-0.009208451956510544,-0.04693159833550453,-0.10632078349590302,0.004949528723955154,0.022808672860264778,0.1983502060174942,-0.02971171960234642,0.08887089043855667,0.025008611381053925,0.04619171470403671,-0.10591808706521988,0.13863417506217957,0.043091922998428345,0.06116897985339165,-0.1449340134859085,-0.0674513727426529,-0.07854432612657547,-0.09132847934961319,-0.03066488914191723,0.04181937873363495,0.008064981549978256,-0.01615755632519722,0.049434319138526917,0.03595075011253357,-0.030811399221420288,0.009601745754480362,0.11508935689926147,0.026917509734630585,-0.07580878585577011,-0.12400998175144196,-0.055754005908966064,-0.003934651613235474,-0.125797837972641,-0.12271388620138168,0.04270177707076073,-0.07857879996299744,0.0105343172326684,-0.22683048248291016,0.01862645335495472,0.020776549354195595,0.09802497923374176,0.12873762845993042,0.11744648963212967,-0.126422718167305,-0.020805897191166878,0.044011104851961136,-0.03622148931026459,-0.04133795201778412,-0.024777771905064583,-0.0744101032614708,-0.11760630458593369,-0.06851860135793686,-0.027419688180088997,-0.023960331454873085,0.17525337636470795,0.09929345548152924,-0.0708637461066246,0.004899164661765099,0.06309307366609573,-0.007948527112603188,-0.001691750017926097,0.007000064942985773,0.11939296126365662,-0.01699530892074108,0.05405938997864723,-0.02993353269994259,0.053190678358078,-0.10431281477212906,-0.08122806251049042,-0.038806766271591187,0.07802007347345352,0.03408902883529663,0.05909526348114014,-0.007547824177891016,0.0009220191859640181,-0.03270347788929939,-0.07333284616470337,-0.0861310139298439,0.06520278006792068,-0.11163410544395447,0.0414322167634964,-0.04557107388973236,0.07040462642908096,-0.04752930998802185,-0.15721063315868378,0.05921800807118416,-0.04696286842226982,0.030369818210601807,0.054824721068143845,0.19889508187770844,0.0018474828684702516,0.007984309457242489,0.0807669535279274,0.04285890981554985,0.009992362931370735,0.044570524245500565,0.05128566175699234,-0.017446238547563553,0.062673419713974,-0.08929932117462158,0.1383594423532486,-0.049085989594459534,0.051332611590623856,0.07341692596673965,-0.00328338285908103,-0.07375012338161469,-0.14933058619499207,-0.16899247467517853,-0.09948911517858505,-0.003958993591368198,-0.015827206894755363,-0.014948124997317791,0.08649393916130066,-0.0766640454530716,0.08102655410766602,0.05404111370444298,0.010648269206285477,0.08212515711784363,-0.06923210620880127,0.038458310067653656,0.01102795172482729,-0.09690407663583755,0.028621068224310875,0.029719607904553413,-0.07236844301223755,0.05981239676475525,0.05973908305168152,0.042825158685445786,0.10001914948225021,0.01368443388491869,-0.06474670022726059,0.07491742819547653,0.039108049124479294,0.024067115038633347,-0.014581511728465557,-0.10282768309116364,0.03179885447025299,-0.019288718700408936,0.2675691246986389,-0.10883515328168869,-0.03455144912004471,0.052078891545534134,0.1278262585401535,0.007907373830676079,0.10682348906993866,-0.0559312142431736,-0.006248817779123783,-0.09896556288003922,-0.13068091869354248,0.011272169649600983,0.019476251676678658,-0.025324370712041855,0.013024609535932541,0.03463888168334961,-0.01592947356402874,0.025334758684039116,-0.06197664141654968,-0.08772111684083939,0.05427258461713791,0.03370970860123634,-0.0061773075722157955,-0.0655362606048584,-0.004848685581237078,0.005459657870233059,0.055946215987205505,-0.12452390789985657,0.12055850774049759,0.029970157891511917,-0.009107902646064758,-0.0011932188645005226,0.05167228728532791,-0.1533443182706833,0.08634209632873535,-0.12241163104772568,0.03066832199692726,-0.08221767842769623,0.0758754089474678,0.012428787536919117,0.01850700005888939,0.09515949338674545,0.03303184732794762,0.10130418837070465,-0.06589576601982117,-0.036575425416231155,-0.06272663176059723,-0.03917628154158592,-0.00909353420138359,0.04117337614297867,-0.03645511344075203,0.04617111757397652,-0.07047034800052643,-0.026266291737556458,-0.0029995206277817488,-0.007861056365072727,-0.006665934808552265,-0.04079023748636246,0.07318910211324692,-0.011335192248225212,-0.03716244176030159,-0.010402624495327473,0.05096350237727165,0.06415968388319016,-0.07144617289304733,0.08351657539606094,-0.01503769401460886,-0.07254177331924438,0.07565119117498398,0.032558273524045944,-0.05867648124694824,0.013431400991976261,-0.025150101631879807,0.04180483892560005,-0.02022249810397625,-0.0758163183927536,-0.054573673754930496,0.11671776324510574,0.04911302775144577,-0.09628608077764511,0.012288767844438553,-0.03951132670044899,-0.013581930659711361,0.048275068402290344,-0.013646237552165985,0.026258621364831924,-0.05415680631995201,-0.06234389916062355,0.1198921874165535,0.13329237699508667,0.20916436612606049,0.10935245454311371,-0.008948123082518578,-0.10868706554174423,0.03954409807920456,0.11971503496170044,0.10026265680789948,-0.12426333874464035,0.027814755216240883,-0.08795247226953506,0.0697317123413086,0.042669959366321564,0.06615792214870453,0.1677125096321106,-0.13197939097881317,-0.00985703244805336,-0.027300775051116943,-0.12512946128845215,-0.07731965184211731,-0.10030850768089294,0.06848196685314178,-0.03993939235806465,0.05902041867375374,-0.1398441046476364,0.039856381714344025,-0.10440944880247116,-0.10816643387079239,0.062464043498039246,-0.04886898025870323,-0.09747303277254105,0.03357478976249695,-0.03762020170688629,0.06992896646261215,0.007194859907031059,0.09293019026517868,0.1364574432373047,-0.11735769361257553,-0.019948089495301247,0.00014998557162471116,-0.1456882804632187,-0.0698915496468544,-0.043624378740787506,-0.03869031369686127,0.08670147508382797,0.08134694397449493,0.0680563747882843,0.0070434375666081905,0.0850488543510437,-0.0013949100393801928,0.08225589990615845,0.062008004635572433,0.05299457907676697,-0.215906023979187,0.060565609484910965,-0.06577195972204208,0.0012292806059122086,-0.06912869215011597,0.07480595260858536,0.019050197675824165,0.03529420867562294,-0.06252053380012512,-0.08500193059444427,0.06062482297420502,-0.06862179189920425,0.045317091047763824,0.018627304583787918,-0.14089274406433105,-0.05154576897621155,0.04543491452932358,0.0018456073012202978,0.09398061782121658,0.22294481098651886,-0.029263410717248917,-0.006915840785950422,0.08957190811634064,0.18263974785804749,-0.008212266489863396,0.03490769490599632,0.0013438076712191105,0.12119970470666885,0.005579454358667135,-0.1901990920305252,-0.062299590557813644,-0.03138532489538193,0.0026981194969266653,-0.1245105192065239,0.028477942571043968,-0.04435689374804497,0.12004270404577255,0.06829851865768433,0.01655019447207451,0.08212519437074661,0.03363557159900665,-0.09298402070999146,0.03786405920982361,0.0273112952709198,0.09107843041419983,-0.13074752688407898,0.03752574697136879,-0.018550416454672813,-0.005245672073215246,-0.03882700204849243,-0.00899677723646164,-0.06278037279844284,0.02470901608467102,-0.024507099762558937,-0.004854544997215271,-0.07930552959442139,-0.03392353653907776,0.06144432723522186,0.03467877581715584,0.06919734179973602,0.00013638829113915563,-0.11641962081193924,0.13428720831871033,-0.08206610381603241,0.11538834869861603,0.06837480515241623,0.049621496349573135,0.10145801305770874,0.05408739298582077,-0.08090665191411972,-0.00971247162669897,0.004962414503097534,0.12988097965717316,0.007460824213922024,0.06016170233488083,0.02358456701040268,-0.09062480926513672,-0.05367439612746239,-0.02672155760228634,0.16088880598545074,-0.027358850464224815,0.059837065637111664,0.11959845572710037,-0.13705894351005554,-0.01471739448606968,-0.06381045281887054,0.06850523501634598,-0.04657071456313133,0.23112325370311737,0.014382975175976753,0.08322514593601227,-0.2249940037727356,-0.07515060156583786,0.1074272021651268,-0.01507116574794054,-0.13789881765842438,0.011817734688520432,0.10775827616453171,0.11600418388843536,0.057428427040576935,0.06415972858667374,0.07093708962202072,0.09626048803329468,-0.09638077020645142,0.03249030560255051,-0.09647145122289658,-0.03924882039427757,-0.014728332869708538,0.01590423472225666,-0.062452130019664764,-0.004262995440512896,-0.06205504387617111,0.0028262983541935682,0.04537695273756981,-0.08026093244552612,-0.1500319540500641,0.029249483719468117,9.463192691328004e-05,-0.018912257626652718,-0.20227791368961334,-0.03276911377906799,-0.11264767497777939,-0.08666188269853592,-0.04099928215146065,-0.01841035485267639,-0.048343218863010406,-0.06402896344661713,0.010041449218988419,-0.07169082760810852,-0.20116406679153442,0.12838725745677948,-0.1819605529308319,-0.07086008042097092,-0.12138823419809341,-0.03015049546957016,-0.0015587829984724522,-0.0282449834048748,-0.047542937099933624,-0.10471782833337784,-0.05317965894937515,-0.05459439009428024,0.05109618604183197,0.11204006522893906,-0.0415518693625927,0.013600679114460945,-0.0507548488676548,-0.033209580928087234,-0.0969567745923996,0.08991926163434982,-0.0030444299336522818,0.05156855657696724,-0.01753731071949005,0.010859377682209015,0.06133696436882019,-0.05053170025348663,0.05708272382616997,-0.06346531957387924,-0.01377103105187416,-0.15709148347377777,-0.10007525235414505,-0.01584203913807869,0.029653901234269142,-0.03235694020986557,0.07412195205688477,0.06670618057250977,-0.04496767744421959,0.041109345853328705,0.0598931759595871,-0.017565827816724777,-0.009051010012626648,0.03610871359705925,-0.0276966355741024,-0.010668263770639896,0.10951026529073715,-0.08048241585493088,-0.025226717814803123,-0.010393429547548294,0.05382348597049713,-0.02701345831155777,0.0795496255159378,0.012433373369276524,0.1886768937110901,-0.03944956883788109,0.08372603356838226,0.10617665946483612,-0.04287762567400932,0.042160119861364365,0.03718963637948036,0.1232757717370987,0.04336113482713699,0.12088151276111603,-0.03504705801606178,0.0762397050857544,-0.0854686051607132,-0.13517101109027863,-0.051382921636104584,-0.01247388031333685,0.12404914945363998,0.03959904983639717,0.14333994686603546,0.03729725629091263,-0.08369681239128113,-0.10044969618320465,-0.057496584951877594,0.05543812736868858,0.11873399466276169,0.09260447323322296,0.08315420895814896,-0.06577831506729126,-0.02549136057496071,-0.021645035594701767,0.02675841934978962,0.03164643421769142,-0.07854059338569641,-0.1469118744134903,-0.2264411598443985,0.040467895567417145,-0.013610548339784145,-0.0028385326731950045,-0.10640391707420349,0.030172305181622505,-0.011194564402103424,-0.10300653427839279,0.04969034716486931,0.06543250381946564,0.05189644172787666,-0.10454443842172623,-0.017182977870106697,-0.014082955196499825,0.07277998328208923,0.042381998151540756,0.16845247149467468,-0.011583840474486351,-0.07117488235235214,0.010989447124302387,-0.05352570861577988,-0.014801770448684692,0.018345236778259277,-0.022784437984228134,-0.05124613642692566,0.08213315159082413,-0.001544538070447743,0.02389330230653286,0.1548483967781067,-0.04825466871261597,0.11966235190629959,-0.031085945665836334,0.12332062423229218,-0.17092417180538177,0.0582590214908123,-0.07487227767705917,-0.06605369597673416,0.13371321558952332,0.012569194659590721,0.17084084451198578,-0.10668707638978958,0.04407224431633949,0.03782786801457405,-0.00745480228215456,-0.08214807510375977,0.10792918503284454,-0.0724092647433281,0.005317146889865398,0.0640205591917038,0.21964144706726074,-0.13047131896018982,0.030700262635946274,0.0766046792268753,-0.009260979481041431,0.11132951080799103,-0.04462791606783867,-0.11012613773345947,0.017135411500930786,0.10217958688735962,0.14868022501468658,0.04560651257634163,-0.15933506190776825,-0.03625025227665901,0.007834755815565586,-0.18177631497383118,-0.0748942568898201,-0.004734504502266645,-0.0021674593444913626,-0.055521510541439056,0.03392425552010536,-0.015259291976690292,0.011472005397081375,0.0068215494975447655,0.018423477187752724,0.011468914337456226,-0.02223121002316475,0.11537797749042511,-0.0003206868714187294,0.05367741361260414,0.04316789284348488,-0.013475523330271244,-0.1519293487071991,0.02409248612821102,-0.08545608818531036,-0.05175480619072914,-0.06935165077447891,-0.10973438620567322,0.03983888030052185,0.15031814575195312,0.15434478223323822,0.10784437507390976,0.10350798070430756,0.01771683059632778,-0.05606265366077423,-0.07824146747589111,0.026104630902409554,-0.02350996620953083,0.10934270173311234,-0.07634712755680084,-0.15583528578281403,-0.052325304597616196,-0.03206869214773178,0.0365685299038887,0.07798376679420471,0.02155386656522751,0.04620186612010002,0.10673835128545761,-0.00895029865205288,-0.08466414362192154,0.009577126242220402,-0.11546612530946732,-0.010725581087172031,-0.04103628545999527,0.022600354626774788,0.0915539488196373,0.054138701409101486,-0.09059203416109085,-0.003834032453596592,0.01793624460697174,-0.055651769042015076,0.005218736827373505,-0.032585788518190384,-0.08694597333669662,-0.18030749261379242,0.051595378667116165,-0.038321353495121,-0.016212789341807365,-0.01728719472885132,0.016430554911494255,-0.0382344052195549,-0.12583371996879578,-0.023076673969626427,0.04155014827847481,0.06304828822612762,-0.010238932445645332,0.05221648886799812,-0.05279608815908432,0.05492808297276497,0.12834475934505463,-0.02550993114709854,-0.1074569970369339,-0.02910209633409977,0.001457483391277492,0.12435892969369888,-0.09900074452161789,-0.07787836343050003,0.057058289647102356,-0.03877547010779381,0.07671535760164261,-0.021019726991653442,0.1692648082971573,-0.11636213213205338,0.059225913137197495,0.13967233896255493,-0.09826312959194183,-0.022914469242095947,-0.024992825463414192,0.018711594864726067,-0.0053518787026405334,0.013501250185072422,-0.11880089342594147,0.05651026591658592,0.15227702260017395,0.11495690047740936,-0.031294066458940506,0.1399107724428177,-0.0032495204359292984,0.002110939472913742,0.010324101895093918,-0.010504573583602905,-0.02748185396194458,-0.07126790285110474,0.06561485677957535,0.04285242408514023,-0.06042534112930298,0.14566804468631744,0.06324061006307602,0.08583585917949677,0.018806852400302887,0.02983815222978592,0.07184527814388275,0.0723351389169693,0.024058986455202103,-0.14121270179748535,-0.01575794257223606,0.029962286353111267,-0.0401790551841259,-0.17917221784591675,0.03373086079955101,0.11179210990667343,0.14237956702709198,-0.006151706911623478,-0.05006254091858864,0.04494191333651543,-0.23004545271396637,-0.043646372854709625,-0.05317404493689537,0.17095260322093964,0.11731065064668655,-0.05168735980987549,-0.06458072364330292,-0.03866232931613922,0.020245244726538658,0.018002547323703766,0.01882384717464447,0.034333486109972,-0.01020755060017109,-0.0003221462538931519,-0.027697721496224403,-0.011639402247965336,0.05211672931909561,0.010828271508216858,0.029223065823316574,0.13025976717472076,0.009800819680094719,-0.037419240921735764,0.09533271193504333,0.00867678876966238,-0.06480468809604645,0.07719825208187103,-0.02781607396900654,-0.07957529276609421,-0.06946398317813873,-0.02426670864224434,0.015647850930690765,-0.1052887812256813,-0.010485825128853321,-0.05036291107535362,-0.09393758326768875,0.0030123682226985693,0.012987713329494,-0.018045196309685707,0.10843849182128906,-0.06396117061376572,-0.03955195099115372,-0.11170034855604172,-0.04604525864124298,0.03911735489964485,-0.0063720764592289925,0.005968443118035793,-0.11352621018886566,-0.030232036486268044,-0.036724869161844254,-0.06867162138223648,-0.016102032735943794,0.031094729900360107,0.08171924948692322,0.04755803942680359,-0.04833704233169556,-0.01548420824110508,0.04770621657371521,0.009456024505198002,0.02544153481721878,-0.0366191565990448,0.07341496646404266,0.040377095341682434,-0.1067095622420311,0.045552194118499756,-0.023035457357764244,0.062047235667705536,-0.11971516162157059,0.08271671831607819,0.05029287189245224,-0.04516439884901047,0.05077141150832176,-0.1199113130569458,-0.020454609766602516,0.012124966830015182,0.011675617657601833,-0.004096883814781904,0.12495872378349304,-0.01140404213219881,0.006318459752947092,-0.06902895867824554,-0.02850702404975891,0.22919400036334991,0.05833373963832855,-0.0935920774936676,-0.06372006982564926,0.027795977890491486,-0.05792730301618576,-0.06800725311040878,0.047127436846494675,0.1932702660560608,0.033774640411138535,-0.13017068803310394,-0.0005456091603264213,0.08698756247758865,-0.03465121239423752,0.033497024327516556,-0.03729354962706566,-0.0648714229464531,0.017650818452239037,-0.03789805248379707,0.0958937406539917,0.06323564797639847,0.19568943977355957,-0.047943152487277985,0.04190649464726448,0.08242867887020111,0.012523851357400417,0.03582684323191643,-0.021796511486172676,0.06649793684482574,-0.004064957611262798,0.006519134156405926,-0.09029140323400497,0.028496894985437393,0.02400357276201248,-0.11763695627450943,0.02472017891705036,-0.041819017380476,0.0006758781964890659,-0.12649162113666534,-0.059038396924734116,-0.002290004165843129,-0.05797107145190239,0.059868671000003815,-0.05051576346158981,0.06379968672990799,0.06453817337751389,-0.032313402742147446,-0.07482609152793884,0.17815445363521576,-0.028551217168569565,0.015271217562258244,0.08803891390562057,0.20497149229049683,0.13689658045768738,-0.17440633475780487,0.08496681600809097,-0.14547331631183624,-0.0006149151595309377,-0.08865692466497421,0.02420903369784355,0.034852463752031326,-0.21806390583515167,0.01972370222210884,0.12021078914403915,-0.04035545513033867,0.03383008763194084,0.07711587846279144,0.03827909007668495,0.07809571921825409,-0.011416458524763584,0.08901418745517731,-0.06863486766815186,0.09477274119853973,-0.12300057709217072,0.1834324449300766,-0.04415231943130493,-0.08716607838869095,0.0539458766579628,-0.0434374175965786,-0.0868648812174797,-0.0002801993687171489,0.005771344061940908,-0.18538057804107666,-0.15152135491371155,-0.011050894856452942,-0.003344521624967456,0.10721908509731293,0.12949900329113007,0.14158962666988373,-0.08703672885894775,0.04855994135141373,0.13666170835494995,0.14154545962810516,-0.061080824583768845,0.007094371132552624,-0.07219365984201431,-0.0552944615483284,0.05153921619057655,0.03159947693347931,-0.14440248906612396,0.019295500591397285,-0.024563279002904892,0.16384486854076385,0.0815727710723877,-0.027055649086833,0.07732068747282028,0.0287687536329031,0.0019611020106822252,0.0075287953950464725,0.028268838301301003,0.06468752771615982,0.21639248728752136,0.04655138775706291,0.03134185075759888,-0.06859251856803894,0.14204183220863342,-0.007363720331341028,-0.030922506004571915,0.06518030166625977,0.06535417586565018,0.036907125264406204,0.09624486416578293,0.18864202499389648,0.05021723359823227,0.03103715181350708,-0.050529126077890396,-0.16226597130298615,-0.05682356655597687,0.05074494332075119,0.017116431146860123,-0.0670279935002327,-0.037829361855983734,0.03102549910545349,-0.010156259872019291,0.061179451644420624,0.13717471063137054,0.14210975170135498,-0.04495687782764435,-0.07274516671895981,0.0513390377163887,-0.15769356489181519,-0.07391539216041565,0.07637642323970795,-0.1428109109401703,0.007364948280155659,-0.040393926203250885,-0.08043422549962997,-0.028160054236650467,-0.011851290240883827,-0.16346845030784607,0.058542050421237946,0.07461105287075043,0.10856704413890839,0.08984782546758652,-0.13049915432929993,-0.07782444357872009,0.04469521716237068,0.028133181855082512,0.014310126192867756,0.08590088784694672,-0.04062891751527786,0.014491223730146885,0.015742260962724686,-0.028882980346679688,-0.07533454149961472,0.10385934263467789,-0.04925854504108429,-0.005093201994895935,-0.05466402694582939,-0.09498845785856247,-0.027684159576892853,-0.04537762328982353,-0.07385814934968948,0.04103889688849449,-0.01599869504570961,0.04502696916460991,-0.03920711204409599,0.029156440868973732,-0.03595038503408432,0.040019676089286804,-0.056771282106637955,0.04889718070626259,-0.05067197233438492,-0.07246214896440506,0.07374513894319534,0.0005584086175076663,-0.09104615449905396,-0.07308195531368256,0.17211103439331055,0.0026661984156817198,-0.020587371662259102,-0.11836732923984528,0.034830980002880096,0.051917560398578644,-0.16227413713932037,-0.15424266457557678,0.010811630636453629,-0.018277646973729134,0.005844566505402327,0.1133807897567749,-0.06474004685878754,-0.0035615197848528624,0.026884546503424644,0.06020588055253029,0.05633166804909706,0.17608994245529175,-0.01911451481282711,-0.03697355091571808,-0.024853378534317017,0.07153773307800293,-0.014476872980594635,-0.12273410707712173,0.09945044666528702,0.14103667438030243,0.012271871790289879,-0.1532006412744522,-0.049999941140413284,0.009581402875483036,0.010540271177887917,-0.020116614177823067,-0.13012154400348663,0.19896194338798523,-0.1387932449579239,0.03548159822821617,0.001472112606279552,-0.012969667091965675,0.0001981816894840449,0.05267266929149628,0.009570226073265076,-0.08878745883703232,0.04836352542042732,0.001495831529609859,0.11523490399122238,0.11026396602392197,-0.04913419857621193,-0.0018344727577641606,-0.031539227813482285,-0.009270229376852512,0.06378327310085297,-0.1507927030324936,-0.038267262279987335,-0.05785166099667549,-0.09959052503108978,-0.00573214003816247,0.027810512110590935,0.10317550599575043,-0.042878977954387665,-0.08597727864980698,-0.03178773447871208,-0.04039490967988968,-0.08530589193105698,-0.04622197523713112,0.11846492439508438,-0.11756455153226852,0.10082423686981201,0.08283238112926483,-0.04596449434757233,-0.04343511909246445,-0.021595535799860954,-0.016891837120056152,0.07689371705055237,-0.014059746637940407,0.069139264523983,0.09103341400623322,-0.1386781632900238,-0.11780750006437302,-0.08970078825950623,0.12305204570293427,-0.003923890646547079,0.02343781664967537,0.09504158794879913,-0.02867315523326397,0.029942499473690987,0.004579450935125351,0.011743149720132351,0.09007436782121658,0.10268469899892807,0.12071793526411057,0.07038029283285141,-0.061001189053058624,-0.07195021212100983,0.053530503064394,0.028883181512355804,-0.04578346386551857,-0.05554905906319618,0.07506415247917175,-0.10685230046510696,0.037243250757455826,-0.003898722119629383,-0.04179960489273071,-0.08712518215179443,-0.021906496956944466,-0.043373920023441315],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){

var gd = document.getElementById('77463ad9-2649-4b0c-83ae-1ea396c953dc');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html>


On the right hand side of plot, we could see there're trump(8.75, 0.02), russias(5.21, 0.05), korea(3.77, 0.08), mayanmar(5.29, 0.1), chinas(4.07, 0.06). And these may be more related to fake news. The reason why this happens is that the topic related to political may easily mislead people.
