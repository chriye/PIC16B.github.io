---
layout: post
title: Blog Post 5
---

In this blog post, we'll introduce tensorflow, data augmentation and transfer learning. 

Tensorflow was developed by Google and it's for deep machine learning on neural networks. In PIC 16A, we used scikit-learn to create and evaluate machine learning models. In fact, tensorflow also able to do that!! To better understand how to use it, there's a task to get started, which is teach a machine learning algorithm to distinguish between pictures of dogs and pictures of cats. 

##  Set up

The tutorial today is using google colab. Later on the post, we'll load images. To speed up the loading, we can go to the 'change the runtime type', and change the 'Hardware accelerator' to GPU on google colab tool's bar.

## 1. Load packages and obtain data

Please copy and paste the following code block.


```python
import os
from tensorflow.keras import utils
```

Later, you will also need to import the folling packages.


```python
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models
```

Please copy and paste it.

In the next code block, it accesses the data from the Tensorflow team that contains labeled images of cats and dogs. Also, it creates train dataset and validation dataset. 


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


### Familiar to the dataset

In order to know the structure of the dataset. Please write a function to output a plots that the first row contains 3 images of cats and the second row contains 3 images of dogs. Add corresponding labels for each image.

To better for you to do the next step, try to run the following code block to obeserve the results.

*train_dataset.take(1)* takes a batch(32 images with label) from the train dataset.


```python
for images, labels in train_dataset.take(1):
  print(labels)
  print(np.where(labels == 0))
```

Here's the function should be like.


```python
def two_row_visualizaation(ds):
  """
  input  : a data set
  output : two rows of pictures
  """
  class_names = ds.class_names
  plt.figure(figsize=(10, 10))
  for images, labels in ds.take(1):
      # get incies for cat
      cat = np.where(labels == 0)[0][:3]
      # get indices for dog
      dog = np.where(labels == 1)[0][:3]
      # combine dog and cat indices
      cat_dog = np.concatenate((cat, dog))
      for i in range(6):
        ax = plt.subplot(2, 3, i + 1)
        plt.imshow(images[cat_dog[i]].numpy().astype("uint8"))
        plt.title(class_names[labels[cat_dog[i]]])
        plt.axis("off")
```

Run and test it.


```python
two_row_visualizaation(train_dataset)
```



![output_11_0.png]({{site.baseurl}}/images/output_11_0.png)
    


Next step, we'll get the frequency of labels. 
The first line of the code block should be added, which creates an iterator called labels.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
# 0 for cat, 1 for dog
freq = {'0': 0, '1': 0}
# count the frequency of 0 and 1
for i in labels_iterator:
  if i == 0:
      freq['0'] += 1
  else:
      freq['1'] += 1

print(freq)
```

    {'0': 1000, '1': 1000}


Please also copy the next code block.
It helps speed up during data reading.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

## 2. First model

Create a *tf.keras.Sequential* model like below. You should notice that it's model1.


```python
model1 = models.Sequential([
    layers.Conv2D(50, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(50, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(50, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(100, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(2)
])
```

This command help you look at the overview of the model1.


```python
model1.summary()

```

    Model: "sequential_2"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_6 (Conv2D)           (None, 158, 158, 50)      1400      
                                                                     
     max_pooling2d_4 (MaxPooling  (None, 79, 79, 50)       0         
     2D)                                                             
                                                                     
     conv2d_7 (Conv2D)           (None, 77, 77, 50)        22550     
                                                                     
     max_pooling2d_5 (MaxPooling  (None, 38, 38, 50)       0         
     2D)                                                             
                                                                     
     conv2d_8 (Conv2D)           (None, 36, 36, 50)        22550     
                                                                     
     flatten_2 (Flatten)         (None, 64800)             0         
                                                                     
     dense_4 (Dense)             (None, 100)               6480100   
                                                                     
     dropout_2 (Dropout)         (None, 100)               0         
                                                                     
     dense_5 (Dense)             (None, 2)                 202       
                                                                     
    =================================================================
    Total params: 6,526,802
    Trainable params: 6,526,802
    Non-trainable params: 0
    _________________________________________________________________


Train model.


```python
model1.compile(optimizer='adam',
               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

history1 = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 106ms/step - loss: 19.1044 - accuracy: 0.5200 - val_loss: 0.6985 - val_accuracy: 0.5681
    Epoch 2/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.5996 - accuracy: 0.6515 - val_loss: 0.7465 - val_accuracy: 0.5792
    Epoch 3/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.4267 - accuracy: 0.8150 - val_loss: 0.9158 - val_accuracy: 0.5953
    Epoch 4/20
    63/63 [==============================] - 7s 114ms/step - loss: 0.2326 - accuracy: 0.9095 - val_loss: 1.0785 - val_accuracy: 0.5817
    Epoch 5/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1693 - accuracy: 0.9455 - val_loss: 1.6661 - val_accuracy: 0.5730
    Epoch 6/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.1368 - accuracy: 0.9525 - val_loss: 1.7696 - val_accuracy: 0.5520
    Epoch 7/20
    63/63 [==============================] - 7s 107ms/step - loss: 0.1127 - accuracy: 0.9640 - val_loss: 1.8864 - val_accuracy: 0.5842
    Epoch 8/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0830 - accuracy: 0.9770 - val_loss: 1.8517 - val_accuracy: 0.5928
    Epoch 9/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0312 - accuracy: 0.9935 - val_loss: 2.7135 - val_accuracy: 0.5990
    Epoch 10/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0629 - accuracy: 0.9825 - val_loss: 1.7972 - val_accuracy: 0.5928
    Epoch 11/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0616 - accuracy: 0.9850 - val_loss: 3.3685 - val_accuracy: 0.5767
    Epoch 12/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0944 - accuracy: 0.9710 - val_loss: 2.3632 - val_accuracy: 0.5842
    Epoch 13/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0396 - accuracy: 0.9880 - val_loss: 2.7376 - val_accuracy: 0.5842
    Epoch 14/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0454 - accuracy: 0.9860 - val_loss: 2.9584 - val_accuracy: 0.6002
    Epoch 15/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0301 - accuracy: 0.9905 - val_loss: 3.9152 - val_accuracy: 0.5866
    Epoch 16/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 3.8288 - val_accuracy: 0.5594
    Epoch 17/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 4.7477 - val_accuracy: 0.5705
    Epoch 18/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.9031 - val_accuracy: 0.5656
    Epoch 19/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0251 - accuracy: 0.9930 - val_loss: 3.9538 - val_accuracy: 0.5619
    Epoch 20/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0634 - accuracy: 0.9780 - val_loss: 3.6833 - val_accuracy: 0.5854


The validating accuracy of my model 1 stabilized **between 55% and 60%** during training.

## 3. Model with data augmentation

Here're we'll use data augmentation, which allows us to give practice with modified images on the same image. The reason why we to this because we still want to able to predict if images are cats or dogs when the images are upside down, etc.

Before we made a model for this, create a *tf.keras.layers.RandomFlip* layer and plots that shows copies of the same image.


```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal_and_vertical')
])
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![output_24_0.png]({{site.baseurl}}/images/output_24_0.png)


Then we also create a *tf.keras.layers.RandomRotation* layer and a plot shows the copies of the same image as well.


```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.2),
])
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![output_27_0.png]({{site.baseurl}}/images/output_27_0.png)


Now, made a model called model2 and add above two layers in it.


```python
model2 = tf.keras.Sequential([ 
    tf.keras.layers.RandomFlip('horizontal_and_vertical'),
    tf.keras.layers.RandomRotation(0.2),
    layers.Conv2D(50, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(50, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(50, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(100, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(2)
])
```


```python
model2.compile(optimizer='adam',
               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

history2 = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 103ms/step - loss: 39.7717 - accuracy: 0.5175 - val_loss: 0.6902 - val_accuracy: 0.5483
    Epoch 2/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.6885 - accuracy: 0.5515 - val_loss: 0.6939 - val_accuracy: 0.5470
    Epoch 3/20
    63/63 [==============================] - 7s 109ms/step - loss: 0.6804 - accuracy: 0.5545 - val_loss: 0.6875 - val_accuracy: 0.5792
    Epoch 4/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.6907 - accuracy: 0.5545 - val_loss: 0.6998 - val_accuracy: 0.5668
    Epoch 5/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.6769 - accuracy: 0.5695 - val_loss: 0.7280 - val_accuracy: 0.5446
    Epoch 6/20
    63/63 [==============================] - 7s 109ms/step - loss: 0.6867 - accuracy: 0.5670 - val_loss: 0.6812 - val_accuracy: 0.5916
    Epoch 7/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.6745 - accuracy: 0.5745 - val_loss: 0.6769 - val_accuracy: 0.5891
    Epoch 8/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.6813 - accuracy: 0.5695 - val_loss: 0.6886 - val_accuracy: 0.5446
    Epoch 9/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.6761 - accuracy: 0.5810 - val_loss: 0.6784 - val_accuracy: 0.5903
    Epoch 10/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.6845 - accuracy: 0.5770 - val_loss: 0.6803 - val_accuracy: 0.5780
    Epoch 11/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.6717 - accuracy: 0.5975 - val_loss: 0.6682 - val_accuracy: 0.5842
    Epoch 12/20
    63/63 [==============================] - 8s 112ms/step - loss: 0.6700 - accuracy: 0.6060 - val_loss: 0.6682 - val_accuracy: 0.5928
    Epoch 13/20
    63/63 [==============================] - 7s 110ms/step - loss: 0.6769 - accuracy: 0.5770 - val_loss: 0.6856 - val_accuracy: 0.5371
    Epoch 14/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.6746 - accuracy: 0.5615 - val_loss: 0.6837 - val_accuracy: 0.5594
    Epoch 15/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.6717 - accuracy: 0.5780 - val_loss: 0.6958 - val_accuracy: 0.5594
    Epoch 16/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.6765 - accuracy: 0.5680 - val_loss: 0.6933 - val_accuracy: 0.5619
    Epoch 17/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.6745 - accuracy: 0.5890 - val_loss: 0.6731 - val_accuracy: 0.5755
    Epoch 18/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.6745 - accuracy: 0.5825 - val_loss: 0.6922 - val_accuracy: 0.5557
    Epoch 19/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.6760 - accuracy: 0.5775 - val_loss: 0.6670 - val_accuracy: 0.5817
    Epoch 20/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.6683 - accuracy: 0.5915 - val_loss: 0.6824 - val_accuracy: 0.5421


The validation accuracy is between 55% and 60%. Compared it to model1, it's pretty similar. And by taking the differences between training accuracy and validation accuracy, there's no overfitting in model2.

## 4. Data preprocessing

Here, we want to modify the data before using it. For instance, we may rescale the RGB number. And sometimes, this method may help to spend less time on getting the weights matrix to fit the data scale.

The following code helps create a preprocessing and please copy it.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, you're ready to create a new model and name it model3. And please add the preprocessor as the first layer in the model3.


```python
model3 = tf.keras.Sequential([ 
    preprocessor,
    tf.keras.layers.RandomFlip('horizontal_and_vertical'),
    tf.keras.layers.RandomRotation(0.2),
    layers.Conv2D(50, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(50, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(60, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(130, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(2)
])
```


```python
model3.compile(optimizer='adam',
               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

history3 = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 8s 113ms/step - loss: 0.8127 - accuracy: 0.5270 - val_loss: 0.6612 - val_accuracy: 0.5792
    Epoch 2/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.6697 - accuracy: 0.5635 - val_loss: 0.6519 - val_accuracy: 0.5718
    Epoch 3/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.6568 - accuracy: 0.5860 - val_loss: 0.6627 - val_accuracy: 0.5780
    Epoch 4/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.6463 - accuracy: 0.5970 - val_loss: 0.6417 - val_accuracy: 0.6139
    Epoch 5/20
    63/63 [==============================] - 7s 109ms/step - loss: 0.6479 - accuracy: 0.5875 - val_loss: 0.6162 - val_accuracy: 0.6374
    Epoch 6/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.6187 - accuracy: 0.6345 - val_loss: 0.6349 - val_accuracy: 0.6312
    Epoch 7/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.6170 - accuracy: 0.6390 - val_loss: 0.6138 - val_accuracy: 0.6547
    Epoch 8/20
    63/63 [==============================] - 8s 119ms/step - loss: 0.5999 - accuracy: 0.6650 - val_loss: 0.6322 - val_accuracy: 0.6559
    Epoch 9/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.6015 - accuracy: 0.6790 - val_loss: 0.5874 - val_accuracy: 0.6832
    Epoch 10/20
    63/63 [==============================] - 8s 118ms/step - loss: 0.5829 - accuracy: 0.6900 - val_loss: 0.5712 - val_accuracy: 0.7129
    Epoch 11/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.5686 - accuracy: 0.7090 - val_loss: 0.5654 - val_accuracy: 0.7129
    Epoch 12/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.5735 - accuracy: 0.6995 - val_loss: 0.5892 - val_accuracy: 0.6918
    Epoch 13/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.5646 - accuracy: 0.6945 - val_loss: 0.5716 - val_accuracy: 0.6980
    Epoch 14/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.5482 - accuracy: 0.7210 - val_loss: 0.5894 - val_accuracy: 0.6720
    Epoch 15/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.5504 - accuracy: 0.7110 - val_loss: 0.5461 - val_accuracy: 0.7327
    Epoch 16/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.5378 - accuracy: 0.7265 - val_loss: 0.5790 - val_accuracy: 0.6955
    Epoch 17/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.5386 - accuracy: 0.7210 - val_loss: 0.6208 - val_accuracy: 0.6448
    Epoch 18/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.5477 - accuracy: 0.6995 - val_loss: 0.5450 - val_accuracy: 0.7079
    Epoch 19/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.5347 - accuracy: 0.7250 - val_loss: 0.5813 - val_accuracy: 0.7054
    Epoch 20/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.5412 - accuracy: 0.7295 - val_loss: 0.5638 - val_accuracy: 0.7191


The validating accuracy is **between 70% and 71%**. Compare to the model1, this prediction is way more accurate!! Using the same method as we used before, it's still a little overfitting.

## 5. Transfer learning

Finally, we made to the last step!! However, this method is a bit different than what we did before. In this model, we're going to use the model that someone has trained before. Sounds like this should be well behaved. Let's check it out!!

The next code block is create from others. So please to copy it.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

Here, create a new model and named it model4. Inside the function, please add the preprocessor layer and the layers from part 3. Additional, please also add base_model_layer from above, Dropout layer with small number and also a Dense(2) layer.


```python
model4 = tf.keras.Sequential([ 
    preprocessor,
    layers.RandomFlip('horizontal_and_vertical'),
    layers.RandomRotation(0.2),
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.15),
    layers.Dense(2)
])
```


```python
model4.summary()
```

    Model: "sequential_19"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_15 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_15 (RandomR  (None, 160, 160, 3)      0         
     otation)                                                        
                                                                     
     model_5 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d_5 (Glo  (None, 1280)             0         
     balMaxPooling2D)                                                
                                                                     
     dropout_17 (Dropout)        (None, 1280)              0         
                                                                     
     dense_26 (Dense)            (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________



```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
history4 = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 117ms/step - loss: 0.8698 - accuracy: 0.7660 - val_loss: 0.1597 - val_accuracy: 0.9480
    Epoch 2/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4984 - accuracy: 0.8765 - val_loss: 0.1549 - val_accuracy: 0.9542
    Epoch 3/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.4749 - accuracy: 0.8830 - val_loss: 0.1106 - val_accuracy: 0.9579
    Epoch 4/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.3929 - accuracy: 0.8910 - val_loss: 0.1068 - val_accuracy: 0.9678
    Epoch 5/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.4002 - accuracy: 0.9030 - val_loss: 0.1040 - val_accuracy: 0.9653
    Epoch 6/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.3433 - accuracy: 0.8990 - val_loss: 0.1028 - val_accuracy: 0.9616
    Epoch 7/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.3313 - accuracy: 0.9065 - val_loss: 0.1014 - val_accuracy: 0.9604
    Epoch 8/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.2898 - accuracy: 0.9125 - val_loss: 0.0833 - val_accuracy: 0.9678
    Epoch 9/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.3395 - accuracy: 0.9015 - val_loss: 0.1475 - val_accuracy: 0.9517
    Epoch 10/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.3320 - accuracy: 0.9140 - val_loss: 0.0736 - val_accuracy: 0.9678
    Epoch 11/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.2792 - accuracy: 0.9155 - val_loss: 0.1017 - val_accuracy: 0.9641
    Epoch 12/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.3320 - accuracy: 0.9015 - val_loss: 0.1107 - val_accuracy: 0.9641
    Epoch 13/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.3081 - accuracy: 0.9115 - val_loss: 0.0860 - val_accuracy: 0.9715
    Epoch 14/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.3089 - accuracy: 0.9105 - val_loss: 0.0837 - val_accuracy: 0.9703
    Epoch 15/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.3104 - accuracy: 0.9060 - val_loss: 0.0795 - val_accuracy: 0.9678
    Epoch 16/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.2409 - accuracy: 0.9240 - val_loss: 0.0925 - val_accuracy: 0.9666
    Epoch 17/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.2680 - accuracy: 0.9160 - val_loss: 0.0942 - val_accuracy: 0.9691
    Epoch 18/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.2586 - accuracy: 0.9095 - val_loss: 0.0866 - val_accuracy: 0.9728
    Epoch 19/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.2840 - accuracy: 0.9200 - val_loss: 0.0759 - val_accuracy: 0.9740
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.2637 - accuracy: 0.9225 - val_loss: 0.0865 - val_accuracy: 0.9678


The validating accuracy is **between 95% and 97%**. And compare to others, this is well trained!! And there's no overfitting on this model!


```python
model4.evaluate(test_dataset, verbose = 2)
```

    6/6 - 1s - loss: 0.0432 - accuracy: 0.9844 - 909ms/epoch - 152ms/step





    [0.04321642592549324, 0.984375]



And it's 98% on testing data.


```python

```
